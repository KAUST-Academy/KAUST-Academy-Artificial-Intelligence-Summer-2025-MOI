{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "![image.png](https://i.imgur.com/a3uAqnb.png)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intro_markdown"
   },
   "source": [
    "# Image Captioning with an Encoder-Decoder RNN\n",
    "\n",
    "This notebook demonstrates how to build an image captioning model using a classic encoder-decoder architecture. This is a fascinating multi-modal task that combines Computer Vision (to understand the image) and Natural Language Processing (to generate a descriptive sentence).\n",
    "\n",
    "### **üìå The Core Idea: Encoder-Decoder**\n",
    "The model consists of two main parts that work together:\n",
    "\n",
    "1.  **The Encoder**: A pre-trained Convolutional Neural Network (CNN), like ResNet, acts as the encoder. It takes an image as input and outputs a compact feature vector (an embedding). This vector is a rich, numerical representation of the image's content‚Äîits \"thought\" vector.\n",
    "\n",
    "2.  **The Decoder**: A Recurrent Neural Network (RNN), specifically an LSTM in our case, acts as the decoder. It takes the image's feature vector from the encoder and generates the caption one word at a time. It learns the statistical structure of language to produce coherent sentences."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2OoUWgrFi82h"
   },
   "source": [
    "from IPython.display import display"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "id": "MN1Z7uGbxTdq"
   },
   "source": [
    "# %pip install torch torchvision pillow spacy numpy\n",
    "# %pip install torchtext\n",
    "# %pip install pycocotools"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import spacy\n",
    "from IPython.display import display\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "id": "5EDyhvMIxybA"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1Ô∏è‚É£ The Dataset: Flickr8k\n",
    "We will use the Flickr8k dataset, a popular benchmark for image captioning. It contains:\n",
    "- **8,000 Images**: Sourced from the Flickr website.\n",
    "- **40,000 Captions**: Each image is paired with five different human-generated captions, providing a rich variety of descriptions.\n",
    "\n",
    "First, we'll download the dataset from Kaggle Hub and inspect its contents."
   ],
   "metadata": {
    "id": "wijrNMlqxi5R"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rlPorEo6jseG"
   },
   "source": [
    "# Download the Flickr8k dataset\n",
    "path = kagglehub.dataset_download(\"adityajn105/flickr8k\")\n",
    "print(\"Path to dataset files:\", path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the captions from the `captions.txt` file\n",
    "captions_path = os.path.join(path, \"captions.txt\")\n",
    "captions_df = pd.read_csv(captions_path)\n",
    "print(f\"Captions loaded: {len(captions_df)} entries\")\n",
    "print(captions_df.head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Images directory\n",
    "images_dir = os.path.join(path, \"Images\")\n",
    "print(f\"Images directory: {images_dir}\")\n",
    "print(f\"Number of images: {len(os.listdir(images_dir))}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Let's look at a sample image and one of its captions\n",
    "print(\"Sample caption:\", captions_df.iloc[0]['caption'])\n",
    "sample_image_path = os.path.join(images_dir, captions_df.iloc[0]['image'])\n",
    "sample_img = Image.open(sample_image_path)\n",
    "display(sample_img)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2Ô∏è‚É£ Building the Vocabulary\n",
    "\n",
    "Neural networks work with numbers, not words. Therefore, we need to build a **vocabulary** to map every unique word in our dataset to a unique integer index. We also add special tokens that are crucial for the training process:\n",
    "\n",
    "- `<PAD>`: A padding token used to make all sequences in a batch the same length.\n",
    "- `<SOS>`: A \"Start of Sentence\" token that signals the beginning of a caption to the decoder.\n",
    "- `<EOS>`: An \"End of Sentence\" token that the model learns to predict when it has finished generating a caption.\n",
    "- `<UNK>`: An \"Unknown\" token for words that appear in the test set but not in our training vocabulary.\n",
    "\n",
    "We use the `spaCy` library for robust and efficient tokenization."
   ],
   "metadata": {
    "id": "pzeLAtu9zQFv"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "I2kg8R_nhN1A"
   },
   "source": [
    "spacy_eng = spacy.load(\"en_core_web_sm\") # you might need to run this: python -m spacy download en_core_web_sm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "y-3CxVtzxcd6"
   },
   "source": [
    "def word_tokenize(text):\n",
    "    return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_JQ8Ow_t7B7P"
   },
   "source": [
    "# Initialize vocabulary with special tokens\n",
    "word_to_index = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
    "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "word_freq = {}\n",
    "\n",
    "# Build vocabulary from all captions in the dataset\n",
    "for caption in tqdm(captions_df['caption']):\n",
    "    tokens = word_tokenize(caption.lower())\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token not in word_to_index:\n",
    "            idx = len(word_to_index)\n",
    "            word_to_index[token] = idx\n",
    "            index_to_word[idx] = token\n",
    "            word_freq[token] = 1\n",
    "        else:\n",
    "            word_freq[token] += 1\n",
    "\n",
    "print(f\"\\nVocabulary size: {len(word_to_index)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Mbv23I3-xUee"
   },
   "source": [
    "word_tokenize('<SOS> hi, my friend <EOS>')  # We will manually add tokens for <EOS> and <SOS> etc after tokenization to avoid them breaking up."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3Ô∏è‚É£ The Model: A CNN-RNN Encoder-Decoder\n",
    "Our model is composed of two main modules: an Encoder to process the image and a Decoder to generate the text."
   ],
   "metadata": {
    "id": "i79_gIk-1zut"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### üîπ The Encoder (CNN)\n",
    "We use a pre-trained ResNet-50 model, which has already learned rich feature representations from the large ImageNet dataset. We perform two key modifications:\n",
    "1.  **Freeze Layers**: We freeze all the convolutional layers (`requires_grad_(False)`) to prevent them from being updated during training. This is a form of transfer learning that speeds up training and prevents overfitting.\n",
    "2.  **Replace Classifier**: We chop off the final, fully-connected classification layer (which originally predicts 1000 ImageNet classes) and replace it with a new `nn.Linear` layer. This new layer will output a feature vector of `embed_size`, which will be the input to our decoder."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### üîπ The Decoder (RNN)\n",
    "The decoder is an LSTM (Long Short-Term Memory) network, a type of RNN well-suited for sequence generation.\n",
    "\n",
    "1.  **Embedding Layer**: It first converts the integer-coded words of the caption into dense vectors of size `embed_size`.\n",
    "2.  **Input Concatenation**: The decoder's first input is the image feature vector from the encoder. For subsequent steps, it takes the embedded representation of the previously generated word.\n",
    "3.  **LSTM**: The LSTM processes the sequence of inputs, maintaining a hidden state that keeps track of the context of the generated sentence.\n",
    "4.  **Linear Layer**: A final linear layer maps the LSTM's output to the size of our vocabulary, producing a probability distribution over all possible next words."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### üîπ The Inference Method (`caption_image`)\n",
    "This method is used for generating new captions after the model is trained. It works *autoregressively*:\n",
    "1.  The image is passed through the encoder to get its feature vector.\n",
    "2.  This feature vector is fed into the decoder to predict the first word (which should be `<SOS>`).\n",
    "3.  The predicted word is then fed back into the decoder as input for the next time step.\n",
    "4.  This process repeats until the model predicts the `<EOS>` token or a maximum length is reached."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "D59I4JSZe46D"
   },
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "\n",
    "        # Use a pretrained ResNet-50 and freeze its parameters\n",
    "        self.resnet = models.resnet50(weights='IMAGENET1K_V1').requires_grad_(False)\n",
    "        # Replace the final fully connected layer\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        return self.dropout(self.relu(features))\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        # LSTM is chosen as the RNN architecture\n",
    "        self.rnn = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 1024),\n",
    "            nn.Linear(1024, vocab_size)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        # Embed the captions and apply dropout\n",
    "        embeddings = self.dropout(self.embed(captions))\n",
    "        # Concatenate image features and caption embeddings\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), dim=1) \n",
    "        hiddens, _ = self.rnn(embeddings)\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class ImageCaptioner(nn.Module):\n",
    "    \"\"\"A wrapper model combining the Encoder and Decoder.\"\"\"\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(ImageCaptioner, self).__init__()\n",
    "        self.encoder = EncoderCNN(embed_size)\n",
    "        self.decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)\n",
    "        outputs = self.decoder(features, captions)\n",
    "        return outputs\n",
    "\n",
    "    def caption_image(self, image, max_length=50):\n",
    "        \"\"\"Autoregressive caption generation for inference.\"\"\"\n",
    "        result_caption = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = self.encoder(image).unsqueeze(1)\n",
    "            states = None\n",
    "\n",
    "            for _ in range(max_length):\n",
    "                hiddens, states = self.decoder.rnn(x, states)\n",
    "                output = self.decoder.linear(hiddens.squeeze(1))\n",
    "                predicted_idx = output.argmax(1)\n",
    "                \n",
    "                predicted_token = index_to_word.get(predicted_idx.item())\n",
    "                if predicted_token is None: # Should not happen\n",
    "                    break\n",
    "                \n",
    "                result_caption.append(predicted_token)\n",
    "                \n",
    "                # Prepare the next input\n",
    "                x = self.decoder.embed(predicted_idx).unsqueeze(1)\n",
    "\n",
    "                if predicted_token == \"<EOS>\":\n",
    "                    break\n",
    "\n",
    "        return result_caption"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4Ô∏è‚É£ The PyTorch Dataset and DataLoader\n",
    "We create a custom `Flickr8kDataset` class to interface with our data. This is the standard PyTorch way to prepare data for a `DataLoader`.\n",
    "\n",
    "The `__getitem__` method is the most important part. For a given index `idx`, it performs these steps:\n",
    "1.  Loads the image and its corresponding caption from our dataframe.\n",
    "2.  Applies the necessary image transformations (resizing to a fixed size, converting to a tensor, and normalizing).\n",
    "3.  Tokenizes the caption, converts it to a sequence of indices, and adds the `<SOS>` and `<EOS>` tokens.\n",
    "4.  Pads the caption tensor with the `<PAD>` token index so that all sequences in a batch have the same length (`max_seq_len`). This is required for batch processing."
   ],
   "metadata": {
    "id": "UVkEmkR_kqU6"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Dsnhr5G5L01M"
   },
   "source": [
    "def convert_sentence_to_idxs(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    idxs = [word_to_index.get(word, word_to_index['<UNK>']) for word in words] \n",
    "    return idxs\n",
    "\n",
    "def convert_idxs_to_sentence(idxs):\n",
    "    words = [index_to_word[idx] for idx in idxs]\n",
    "    return ' '.join(words)\n",
    "\n",
    "class Flickr8kDataset(Dataset):\n",
    "    def __init__(self, captions_df, images_dir, max_seq_len, transform):\n",
    "        self.captions_df = captions_df\n",
    "        self.images_dir = images_dir\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.captions_df.iloc[idx]\n",
    "        \n",
    "        # Load image\n",
    "        img_path = os.path.join(self.images_dir, row['image'])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        # Process caption\n",
    "        caption = row['caption'].lower().strip()\n",
    "        caption_idxs = convert_sentence_to_idxs(caption)\n",
    "        \n",
    "        # Truncate if caption is too long\n",
    "        if len(caption_idxs) > self.max_seq_len - 2:  # 2 for SOS and EOS\n",
    "            caption_idxs = caption_idxs[:self.max_seq_len-2]\n",
    "\n",
    "        # Calculate padding length\n",
    "        padding_len = self.max_seq_len - len(caption_idxs) - 2\n",
    "\n",
    "        # Create the final tensor with special tokens and padding\n",
    "        caption_tensor = (\n",
    "            [word_to_index['<SOS>']] +\n",
    "            caption_idxs +\n",
    "            [word_to_index['<EOS>']] +\n",
    "            [word_to_index['<PAD>']] * padding_len\n",
    "        )\n",
    "\n",
    "        caption_tensor = torch.tensor(caption_tensor, dtype=torch.long)\n",
    "        return img, caption_tensor"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) # ImageNet stats\n",
    "])\n",
    "\n",
    "# For demonstration purposes, we'll train on a small subset of the data.\n",
    "# Increase this for better results!\n",
    "train_size = min(5000, len(captions_df))\n",
    "train_captions = captions_df.head(train_size)\n",
    "\n",
    "batch_size = 32\n",
    "max_seq_len = 50 # Max caption length\n",
    "\n",
    "train_dataset = Flickr8kDataset(train_captions, images_dir, max_seq_len=max_seq_len, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "print(f\"Training on {len(train_dataset)} image-caption pairs\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rlsyakwKrQ2Z"
   },
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5Ô∏è‚É£ Model Training\n",
    "Now we define the hyperparameters, initialize the model, and set up the optimizer and loss function for training."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### üîπ Hyperparameters & Setup\n",
    "These are the key settings for our model and training process. Tuning these can significantly impact performance."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "embed_size = 512\n",
    "hidden_size = 256\n",
    "vocab_size = len(word_to_index)\n",
    "num_decoder_layers = 3\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 15 # Increased for better results, but still short for a full training"
   ],
   "metadata": {
    "id": "5YB-RYSH2DmL"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### üîπ Loss Function and Optimizer\n",
    "- **Loss Function**: We use `nn.CrossEntropyLoss`. This is the standard choice for multi-class classification, which is what we're doing at each time step‚Äîpredicting the next word from the entire vocabulary.\n",
    "Importantly, we set `ignore_index=word_to_index['<PAD>']`. This tells the loss function to ignore the `<PAD>` tokens when calculating the loss, so the model isn't penalized for its predictions on padded parts of the sequence.\n",
    "\n",
    "- **Optimizer**: We use the Adam optimizer, a popular and effective choice for deep learning models."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "v7-7f4gmhr5u"
   },
   "source": [
    "# Initialize model, loss, and optimizer\n",
    "model = ImageCaptioner(embed_size, hidden_size, vocab_size, num_decoder_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word_to_index['<PAD>']) \n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Only finetune the final layer of the CNN encoder\n",
    "for name, param in model.encoder.resnet.named_parameters():\n",
    "    if \"fc.weight\" in name or \"fc.bias\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Vocabulary size: {len(word_to_index)}\")\n",
    "print(f\"Sample vocabulary: {list(word_to_index.keys())[:20]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"SOS token: {word_to_index['<SOS>']}\")\n",
    "print(f\"EOS token: {word_to_index['<EOS>']}\")\n",
    "print(f\"PAD token: {word_to_index['<PAD>']}\")\n",
    "print(f\"UNK token: {word_to_index['<UNK>']}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### üîπ Pre-training Check\n",
    "Before we start training, let's see what our randomly initialized model produces. We expect the captions to be nonsensical."
   ],
   "metadata": {
    "id": "Db6Iq3hU3GMk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Prepare a few random test samples\n",
    "test_samples = []\n",
    "test_images = []\n",
    "\n",
    "random_indices = random.sample(range(len(captions_df)), min(3, len(captions_df)))\n",
    "\n",
    "for i in random_indices:\n",
    "    row = captions_df.iloc[i]\n",
    "    img_path = os.path.join(images_dir, row['image'])\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    \n",
    "    test_samples.append({'image': img, 'caption': row['caption']})\n",
    "    img_tensor = transform(img).to(device)\n",
    "    test_images.append(img_tensor)"
   ],
   "metadata": {
    "id": "QtS8hnpH3K3D"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xiAYu3QXtHki"
   },
   "source": [
    "print(\"--- Pre-training captions ---\")\n",
    "model.eval()\n",
    "for i, sample in enumerate(test_samples):\n",
    "    display(sample['image'])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # The model's `caption_image` expects a batch, so we add a dimension with unsqueeze(0)\n",
    "        caption = model.caption_image(test_images[i].unsqueeze(0))\n",
    "        caption_text = ' '.join(caption)\n",
    "        print(f\"Generated: {caption_text}\")\n",
    "        print(f\"Original: {sample['caption']}\")\n",
    "        print(\"-\" * 50)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### üîπ The Training Loop\n",
    "The training process involves iterating through the dataset for a number of epochs. In each step:\n",
    "\n",
    "1.  **Teacher Forcing**: We feed the ground-truth caption (shifted by one position) into the decoder at each time step, regardless of the decoder's own prediction from the previous step. This is a technique called *teacher forcing*, which stabilizes training and helps the model learn the language structure more quickly. We feed `captions[:, :-1]` (from `<SOS>` to the second-to-last word) and expect the model to predict `captions[:, 1:]` (from the first word to `<EOS>`).\n",
    "2.  **Calculate Loss**: The model's output (logits) is compared against the target sequence to calculate the loss.\n",
    "3.  **Backpropagation**: The loss is backpropagated through the network to compute gradients.\n",
    "4.  **Gradient Clipping**: We clip the gradients (`clip_grad_norm_`) to a maximum value. This is a common practice in RNN training to prevent the \"exploding gradients\" problem.\n",
    "5.  **Optimizer Step**: The optimizer updates the model's trainable parameters (the decoder and the final layer of the encoder)."
   ],
   "metadata": {
    "id": "oOQLLt_F3iIs"
   }
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "epoch_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for idx, (imgs, captions) in tqdm(\n",
    "        enumerate(train_loader), total=len(train_loader), leave=False,\n",
    "        desc=f\"Epoch {epoch+1}/{num_epochs}\"\n",
    "    ):\n",
    "        imgs = imgs.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        # Teacher forcing: feed the ground-truth captions (shifted) to the decoder\n",
    "        outputs = model(imgs, captions[:, :-1]) \n",
    "        \n",
    "        # We skip the first prediction, which corresponds to the image feature input.\n",
    "        outputs = outputs[:, 1:, :]\n",
    "        targets = captions[:, 1:]\n",
    "        \n",
    "        # Reshape for CrossEntropyLoss and calculate loss\n",
    "        loss = criterion(\n",
    "            outputs.reshape(-1, outputs.shape[2]),  # Shape: [batch_size * (seq_len-1), vocab_size]\n",
    "            targets.flatten()                       # Shape: [batch_size * (seq_len-1)]\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Gradient clipping\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    epoch_losses.append(avg_loss) # Save loss for plotting\n",
    "    print(f'Epoch: {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    # Test generation every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.eval()\n",
    "        print(f\"\\n--- Testing after epoch {epoch+1} ---\")\n",
    "        with torch.no_grad():\n",
    "            test_caption = model.caption_image(test_images[0].unsqueeze(0))\n",
    "            print(f\"Generated: {' '.join(test_caption)}\")\n",
    "            print(f\"Original: {test_samples[0]['caption']}\")\n",
    "        print(\"-\" * 50)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### üîπ Plotting the Training Progress\n",
    "Visualizing the loss helps us understand how well the model is learning. A decreasing loss indicates that the model is successfully minimizing its prediction error over time."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(epoch_losses) + 1), epoch_losses, 'o-', label='Training Loss')\n",
    "plt.title('Training Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6Ô∏è‚É£ Saving and Loading the Model\n",
    "\n",
    "To avoid retraining the model every time we want to use it, we can save its state. It's good practice to save three things:\n",
    "1.  **Model State Dictionary**: The learned weights and biases of the model.\n",
    "2.  **Vocabulary Mappings**: The `word_to_index` and `index_to_word` dictionaries are essential for converting between words and indices during inference.\n",
    "3.  **Model Configuration**: The hyperparameters used to build the model (`embed_size`, `hidden_size`, etc.), so we can perfectly reconstruct the architecture before loading the weights."
   ],
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def save_model(model, word_to_index, index_to_word, model_config, save_dir=\"flickr8k_rnn_image_captioning_dataset\"):\n",
    "    \"\"\"\n",
    "    Save the trained model, vocabulary, and configuration to a specified directory.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model state dict\n",
    "    model_path = os.path.join(save_dir, 'flickr8k_image_captioner.pth')\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model state dict saved to '{model_path}'\")\n",
    "    \n",
    "    # Save vocabulary mappings\n",
    "    vocab_data = {\n",
    "        'word_to_index': word_to_index,\n",
    "        'index_to_word': index_to_word,\n",
    "        'vocab_size': len(word_to_index)\n",
    "    }\n",
    "    vocab_path = os.path.join(save_dir, 'vocab_mappings.pkl')\n",
    "    with open(vocab_path, 'wb') as f:\n",
    "        pickle.dump(vocab_data, f)\n",
    "    print(f\"Vocabulary mappings saved to '{vocab_path}'\")\n",
    "    \n",
    "    # Save model configuration\n",
    "    config_path = os.path.join(save_dir, 'model_config.pkl')\n",
    "    with open(config_path, 'wb') as f:\n",
    "        pickle.dump(model_config, f)\n",
    "    print(f\"Model configuration saved to '{config_path}'\")\n",
    "    \n",
    "    print(f\"\\nAll files saved successfully in '{save_dir}' directory!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the model configuration to be saved\n",
    "model_config = {\n",
    "    'embed_size': embed_size,\n",
    "    'hidden_size': hidden_size,\n",
    "    'vocab_size': vocab_size,\n",
    "    'num_decoder_layers': num_decoder_layers\n",
    "}\n",
    "\n",
    "# Save the current trained model\n",
    "save_model(model, word_to_index, index_to_word, model_config)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_model(save_dir=\"flickr8k_rnn_image_captioning_dataset\", device='cpu'):\n",
    "    \"\"\"\n",
    "    Load the trained model, vocabulary, and configuration from a specified directory.\n",
    "    \"\"\"\n",
    "    # Load vocabulary mappings\n",
    "    vocab_path = os.path.join(save_dir, 'vocab_mappings.pkl')\n",
    "    with open(vocab_path, 'rb') as f:\n",
    "        vocab_data = pickle.load(f)\n",
    "    print(f\"Vocabulary mappings loaded from '{vocab_path}'\")\n",
    "    \n",
    "    # Load model configuration\n",
    "    config_path = os.path.join(save_dir, 'model_config.pkl')\n",
    "    with open(config_path, 'rb') as f:\n",
    "        config = pickle.load(f)\n",
    "    print(f\"Model configuration loaded from '{config_path}'\")\n",
    "    \n",
    "    # Recreate model architecture using the loaded config\n",
    "    model = ImageCaptioner(\n",
    "        config['embed_size'], \n",
    "        config['hidden_size'], \n",
    "        config['vocab_size'], \n",
    "        config['num_decoder_layers']\n",
    "    )\n",
    "    \n",
    "    # Load model weights\n",
    "    model_path = os.path.join(save_dir, 'flickr8k_image_captioner.pth')\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    print(f\"Model weights loaded from '{model_path}'\")\n",
    "    \n",
    "    print(f\"\\nModel loaded successfully from '{save_dir}' directory!\")\n",
    "    return model, vocab_data['word_to_index'], vocab_data['index_to_word']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example of loading the model\n",
    "# We also reload the vocabulary to ensure consistency, especially if running this section independently\n",
    "loaded_model, word_to_index_loaded, index_to_word_loaded = load_model(device=device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7Ô∏è‚É£ Generating Captions (Inference)\n",
    "Finally, let's see our trained model in action! We will use the same test images from our pre-training check and compare the new, hopefully much-improved, captions.\n",
    "\n",
    "The model is set to evaluation mode (`model.eval()`) to disable layers like Dropout that are only used during training."
   ],
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model.eval() # Ensure model is in evaluation mode\n",
    "\n",
    "print(\"\\n--- Post-training captions ---\")\n",
    "\n",
    "for i, sample in enumerate(test_samples):\n",
    "    display(sample['image'])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        caption = model.caption_image(test_images[i].unsqueeze(0))\n",
    "        caption_text = ' '.join(caption)\n",
    "        print(f\"Image {i+1}:\")\n",
    "        print(f\"  Generated: {caption_text}\")\n",
    "        print(f\"  Original:  {sample['caption']}\")\n",
    "        print(\"-\" * 50)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Conclusion & Next Steps\n",
    "\n",
    "We have successfully built, trained, and evaluated an image captioning model using a CNN encoder and an RNN decoder. While the results after a short training run are promising, they can be significantly improved.\n",
    "\n",
    "### **üìù Exercises for Further Exploration**\n",
    "1.  **More Training**: The most impactful change would be to train for more `epochs` on a larger portion of the dataset (or the full dataset). Deep learning models are data-hungry!\n",
    "2.  **Hyperparameter Tuning**: Experiment with different values for `embed_size`, `hidden_size`, `num_decoder_layers`, and `learning_rate`.\n",
    "3.  **Different RNNs**: Replace the `nn.LSTM` with a `nn.GRU` in the decoder. Compare the results and training time.\n",
    "4.  **Attention Mechanism**: The biggest architectural improvement for this task is to add an **attention mechanism**. Attention allows the decoder to dynamically focus on different parts of the image as it generates each word, often leading to more accurate and contextually relevant captions. This is a key component in modern sequence-to-sequence models.\n",
    "5.  **Beam Search Decoding**: The current `caption_image` method uses greedy decoding (picking the single best word at each step). Implement **beam search**, which keeps track of the `k` most probable sequences at each step, often resulting in higher-quality final captions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Contributed by: Ali Habibullah(Changed from Telha Bilal original notebook ü´°)"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
