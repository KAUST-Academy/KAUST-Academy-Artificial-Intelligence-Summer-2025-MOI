{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fee4e5c1",
   "metadata": {},
   "source": [
    "![image.png](https://i.imgur.com/a3uAqnb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d4aa2d9dd1cbf8",
   "metadata": {
    "id": "2Hu5LPRgr152"
   },
   "source": [
    "# üé® Conditional Image Generation with cGANs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e60a21aa8ad0cfa",
   "metadata": {
    "id": "Jc1X0DEysUuO"
   },
   "source": [
    "While standard GANs learn to generate realistic images from a dataset, they offer no control over *what kind* of image is generated. The output is entirely random.\n",
    "\n",
    "**Conditional GANs (cGANs)** solve this by introducing a **condition** (`y`) to the generation process. This condition, which could be a class label, text, or a set of attributes, is fed into both the generator and the discriminator.\n",
    "\n",
    "- üë®‚Äçüé® **The Conditional Generator** learns to create 'fake' images that not only look real but also match the given condition `y`.\n",
    "- üïµÔ∏è‚Äç‚ôÇÔ∏è **The Conditional Discriminator** learns to determine if an image is real *and* if it matches its corresponding condition `y`.\n",
    "\n",
    "This allows us to direct the image generation process. For this notebook, we'll use the CelebA dataset and condition the GAN on facial attributes like 'Smiling', 'Male', and 'Blond_Hair' to generate specific types of faces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d2c786084d46d7",
   "metadata": {
    "id": "sxG6IitzCCqg"
   },
   "source": [
    "![GANs vs CGANs Image](https://i.imgur.com/PNRcz5N.png) \n",
    "<p align = \"center\">\n",
    "Fig.1 - GAN vs. CGAN \n",
    "(<a href=\"https://itchef.ru/articles/341494/\">\n",
    "source\n",
    "</a>)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9337c06f219c166d",
   "metadata": {
    "id": "VSOwMH3CO8WX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "import pandas as pd\n",
    "\n",
    "from torchvision.transforms import Compose, ToTensor, Lambda, Resize, Normalize\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import kagglehub\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d52708dde91550",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Hyperparameters and Setup\n",
    "Here we define the key parameters for our cGAN model and training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a6cfdb5e18669c",
   "metadata": {
    "id": "4jhd_YH5vAHj"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5 # Training for longer can improve results, I found 150 the best for this task.\n",
    "LEARNING_RATE = 2e-4\n",
    "BATCH_SIZE = 128\n",
    "Z = 100 # Latent dimension\n",
    "IMAGE_SIZE = 64\n",
    "NUM_ATTRIBUTES = 5 # Number of conditional attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf22d3ce9c0095d7",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Loading the CelebA Dataset with Attributes\n",
    "\n",
    "We'll use the CelebA (CelebFaces Attributes) dataset, which is perfect for this task because it includes a large collection of celebrity faces, each annotated with 40 binary attributes.\n",
    "\n",
    "### üîπ **Dataset Details:**\n",
    "- **Source**: `jessicali9530/celeba-dataset` from Kaggle\n",
    "- **Content**: Over 200,000 celebrity images with attribute annotations.\n",
    "- **Conditioning**: We will select 5 key attributes to condition our model on: `Male`, `Smiling`, `Young`, `Eyeglasses`, and `Blond_Hair`.\n",
    "- **Preprocessing**: Images are resized to `64x64` and pixel values are normalized to `[-1, 1]` to match the generator's `Tanh` output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dde544ae07293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms to resize and normalize the images\n",
    "transform_steps = Compose([\n",
    "    Resize((IMAGE_SIZE, IMAGE_SIZE)), \n",
    "    ToTensor(),\n",
    "    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # Normalize to [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913950aca50e72b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class for CelebA faces and attributes\n",
    "class CelebADataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, max_samples=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Load the main attributes file\n",
    "        attr_file = os.path.join(root_dir, 'list_attr_celeba.csv')\n",
    "        self.attr_df = pd.read_csv(attr_file)\n",
    "        \n",
    "        # Define the 5 key attributes we'll use for conditioning\n",
    "        self.selected_attributes = ['Male', 'Smiling', 'Young', 'Eyeglasses', 'Blond_Hair']\n",
    "        \n",
    "        # The image directory is nested, so we define the full path\n",
    "        self.img_dir = os.path.join(root_dir, 'img_align_celeba', 'img_align_celeba')\n",
    "        \n",
    "        self.image_files = []\n",
    "        self.attributes = []\n",
    "        \n",
    "        count = 0\n",
    "        # Iterate through the attribute file to get image names and their labels\n",
    "        for idx, row in tqdm(self.attr_df.iterrows(), total=len(self.attr_df)):\n",
    "            if max_samples and count >= max_samples:\n",
    "                break\n",
    "                \n",
    "            img_filename = row['image_id']\n",
    "            img_path = os.path.join(self.img_dir, img_filename)\n",
    "\n",
    "            # Only add the file if it actually exists\n",
    "            if os.path.exists(img_path):\n",
    "                self.image_files.append(img_path)\n",
    "                # The attributes are stored as -1 and 1. We convert them to 0 and 1.\n",
    "                attrs = [(row[attr] + 1) // 2 for attr in self.selected_attributes]\n",
    "                self.attributes.append(attrs)\n",
    "                count += 1\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        # Get the attributes and convert them to a tensor\n",
    "        attributes = torch.tensor(self.attributes[idx], dtype=torch.long)\n",
    "\n",
    "        # Open the image and convert to RGB\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        # Return both the image and its corresponding attributes\n",
    "        return image, attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb625c147f72bc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the CelebA dataset from Kaggle\n",
    "path = kagglehub.dataset_download(\"jessicali9530/celeba-dataset\")\n",
    "\n",
    "# Create the dataset object. We use a subset for faster training.\n",
    "celeba_dataset = CelebADataset(root_dir=path, transform=transform_steps, max_samples=40000)\n",
    "attribute_names = celeba_dataset.selected_attributes\n",
    "\n",
    "print(f\"\\nDataset size: {len(celeba_dataset)}\")\n",
    "print(f\"Selected attributes: {attribute_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbc44da19a33729",
   "metadata": {
    "id": "RCqfqsOB45lv"
   },
   "outputs": [],
   "source": [
    "# Create the DataLoader\n",
    "train_dataloader = DataLoader(celeba_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffe776f63599698",
   "metadata": {},
   "source": [
    "### üîπ Visualizing the Data\n",
    "Let's look at some examples from the dataset to see the images and their associated attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd80f5b1a4c300b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to format the attribute tensor into a readable string for plotting\n",
    "def format_attributes(attrs):\n",
    "    \"\"\"Convert attribute tensor to readable string\"\"\"\n",
    "    attr_str = []\n",
    "    for i, attr in enumerate(attrs):\n",
    "        if attr == 1:\n",
    "            attr_str.append(attribute_names[i])\n",
    "    return \", \".join(attr_str) if attr_str else \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293f15080cb6161b",
   "metadata": {
    "id": "sL7l46Fk47y5"
   },
   "outputs": [],
   "source": [
    "# Visualizing some sample images\n",
    "figure = plt.figure(figsize=(15, 5))\n",
    "cols, rows = 7, 1\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(celeba_dataset), size=(1,)).item()\n",
    "    img, attrs = celeba_dataset[sample_idx]\n",
    "    # De-normalize from [-1,1] to [0,1] for display\n",
    "    img_display = (img + 1) / 2 \n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(format_attributes(attrs), fontsize=8)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img_display.permute((1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8e046dadcd038d",
   "metadata": {
    "id": "71kmnnwN49Qo"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6e63d6da528f",
   "metadata": {
    "id": "sIaJSsev1WfC"
   },
   "source": [
    "## Building the Conditional GAN (cGAN) Models\n",
    "\n",
    "Our cGAN consists of two neural networks. The key difference from a standard GAN is that both networks now take the conditional attributes as an additional input. We will follow best practices from the DCGAN paper to stabilize training:\n",
    "\n",
    "- **Weight Initialization**: Initialize weights from a Normal distribution with `mean=0`, `stdev=0.02`.\n",
    "- **Activations**: Use `LeakyReLU` to prevent dying gradients.\n",
    "- **Optimizer**: Use the `Adam` optimizer with `beta1=0.5`.\n",
    "- **Attribute Handling**: We use `nn.Embedding` layers to convert our binary attributes (0 or 1) into dense vectors, which are easier for the network to process.\n",
    "\n",
    "--- \n",
    "\n",
    "### üë®‚Äçüé® The Conditional Generator\n",
    "\n",
    "The Generator's job is to create believable images that match a given set of attributes.\n",
    "\n",
    "**Architecture:**\n",
    "1.  **Input**: It takes two inputs: a 100-dimensional noise vector (`Z`) and a 5-element attribute vector.\n",
    "2.  **Embedding**: Each of the 5 attributes is passed through its own `nn.Embedding` layer. The resulting embedding vectors are concatenated together.\n",
    "3.  **Concatenation**: The latent noise vector `Z` is concatenated with the final attribute embedding vector.\n",
    "4.  **Upsampling**: This combined vector is fed through a series of `ConvTranspose2d` layers to upsample it into a `64x64` color image.\n",
    "5.  **Output**: A `Tanh` activation function ensures the output image pixels are in the range `[-1, 1]`, matching our input data.\n",
    "\n",
    "--- \n",
    "\n",
    "### üïµÔ∏è‚Äç‚ôÇÔ∏è The Conditional Discriminator\n",
    "\n",
    "The Discriminator's job is to classify an image as real or fake, *given* a set of attributes.\n",
    "\n",
    "**Architecture:**\n",
    "1.  **Input**: It takes two inputs: a `64x64x3` image and the 5-element attribute vector.\n",
    "2.  **Image Path**: The image is passed through a series of `Conv2d` layers to extract features and downsample it.\n",
    "3.  **Attribute Path**: The attributes are passed through `nn.Embedding` layers, just like in the generator.\n",
    "4.  **Concatenation**: The flattened feature vector from the image is concatenated with the attribute embedding vector.\n",
    "5.  **Output**: This combined vector is passed through fully connected layers to produce a single `Sigmoid` output between 0 (fake or mismatched) and 1 (real and matched)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f47a4b96d41822",
   "metadata": {
    "id": "h2BjiUtq4-yF"
   },
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    \"\"\"A custom reshape layer.\"\"\"\n",
    "    def __init__(self, shape):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(*self.shape)\n",
    "\n",
    "def weights_init(m):\n",
    "    \"\"\"Reinitialize model weights. GAN authors recommend them to be sampled from N(0,0.2)\"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "        \n",
    "        \n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Generator model for CelebA with 5 attributes conditioning\"\"\"\n",
    "    def __init__(self, z, num_attributes, attr_embedding_size=5):\n",
    "        super(Generator, self).__init__()\n",
    "        self.z = z\n",
    "        self.num_attributes = num_attributes\n",
    "        self.total_attr_embedding_size = num_attributes * attr_embedding_size\n",
    "        \n",
    "        # Embedding layer for each attribute (5 attributes, each with embedding size 5)\n",
    "        self.condition_embeddings = nn.ModuleList([\n",
    "            nn.Embedding(2, attr_embedding_size) for _ in range(num_attributes) # Binary attributes (0 or 1)\n",
    "        ])\n",
    "\n",
    "        self.gen_model = nn.Sequential(\n",
    "            # Combined input: Z + attribute embeddings\n",
    "            nn.Linear(z + self.total_attr_embedding_size, 512 * 4 * 4),\n",
    "            nn.BatchNorm1d(512 * 4 * 4),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            Reshape((-1, 512, 4, 4)),\n",
    "            \n",
    "            # 4x4 -> 8x8\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # 8x8 -> 16x16\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # 16x16 -> 32x32\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # 32x32 -> 64x64\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh() # Output normalized to [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, conditions):\n",
    "        conditions = conditions.long()\n",
    "        \n",
    "        # Create embeddings for each of the 5 attributes\n",
    "        condition_embeds = []\n",
    "        for i in range(self.num_attributes):\n",
    "            embed = self.condition_embeddings[i](conditions[:, i])\n",
    "            condition_embeds.append(embed)\n",
    "        \n",
    "        # Concatenate all attribute embeddings\n",
    "        condition_embed = torch.cat(condition_embeds, dim=1)\n",
    "        \n",
    "        # Concatenate the noise vector 'x' and the condition embeddings\n",
    "        combined_input = torch.cat([x, condition_embed], dim=1)\n",
    "        return self.gen_model(combined_input)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator model for CelebA with 5 attributes conditioning\"\"\"\n",
    "    def __init__(self, num_attributes, attr_embedding_size=5):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.num_attributes = num_attributes\n",
    "        self.total_attr_embedding_size = num_attributes * attr_embedding_size\n",
    "\n",
    "        # CNN part for image feature extraction\n",
    "        self.image_feature_extractor = nn.Sequential(\n",
    "            # 64x64 -> 32x32\n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 32x32 -> 16x16\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 16x16 -> 8x8\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 8x8 -> 4x4\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Embedding layers for the conditions\n",
    "        self.condition_embeddings = nn.ModuleList([\n",
    "            nn.Embedding(2, attr_embedding_size) for _ in range(num_attributes)\n",
    "        ])\n",
    "        \n",
    "        # Classifier part\n",
    "        self.classifier = nn.Sequential(\n",
    "            # Input is flattened image features + concatenated attribute embeddings\n",
    "            nn.Linear(512 * 4 * 4 + self.total_attr_embedding_size, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, conditions):\n",
    "        conditions = conditions.long()\n",
    "\n",
    "        # Extract features from the image\n",
    "        image_features = self.image_feature_extractor(x)\n",
    "        \n",
    "        # Create embeddings for each attribute\n",
    "        condition_embeds = []\n",
    "        for i in range(self.num_attributes):\n",
    "            embed = self.condition_embeddings[i](conditions[:, i])\n",
    "            condition_embeds.append(embed)\n",
    "        \n",
    "        # Concatenate all attribute embeddings\n",
    "        condition_embed = torch.cat(condition_embeds, dim=1)\n",
    "        \n",
    "        # Concatenate image features and condition embeddings\n",
    "        combined_features = torch.cat([image_features, condition_embed], dim=1)\n",
    "        \n",
    "        # Classify the combined features\n",
    "        output = self.classifier(combined_features)\n",
    "        return output.view(-1, 1).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3705df009d7a1fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(z=Z, num_attributes=NUM_ATTRIBUTES)\n",
    "generator.apply(weights_init)\n",
    "generator.to(device)\n",
    "\n",
    "discriminator = Discriminator(num_attributes=NUM_ATTRIBUTES)\n",
    "discriminator.apply(weights_init)\n",
    "discriminator.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e927fbe96efc5f",
   "metadata": {},
   "source": [
    "### Model Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34ac7de7bb8fe4e",
   "metadata": {},
   "source": [
    "### üéØ Loss Function and Optimizers\n",
    "\n",
    "- **Loss**: We use **Binary Cross-Entropy Loss** (`BCELoss`), which is ideal for the discriminator's binary classification task (real/matched vs. fake/mismatched).\n",
    "- **Optimizers**: We use the `Adam` optimizer for both networks with a `beta1` of 0.5, as recommended for stable GAN training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd213878e68dc14",
   "metadata": {
    "id": "4rjJDKZhG00D"
   },
   "outputs": [],
   "source": [
    "# Defining the optimizer and loss function here\n",
    "optimizer_gen = torch.optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "optimizer_disc = torch.optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aff4d2684d9129",
   "metadata": {
    "id": "VdLi75kxNCOV"
   },
   "outputs": [],
   "source": [
    "real_label = 1\n",
    "fake_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de061d1118c64d06",
   "metadata": {
    "id": "nQFU-oAONECz"
   },
   "outputs": [],
   "source": [
    "def display_image_grid(images, labels, num_rows, num_cols, title_text):\n",
    "    fig = plt.figure(figsize=(num_cols*2., num_rows*2.))\n",
    "    grid = ImageGrid(fig, 111, nrows_ncols=(num_rows, num_cols), axes_pad=0.25)\n",
    "\n",
    "    for ax, im, attrs in zip(grid, images, labels):\n",
    "        im = torch.clamp(im, 0, 1) # Ensure values are in [0,1] for display\n",
    "        ax.imshow(im.permute(1,2,0))\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(format_attributes(attrs), fontsize=7)\n",
    "    \n",
    "    fig.suptitle(title_text, fontsize=16, y=1.05)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285238e54e6a0865",
   "metadata": {},
   "source": [
    "## üöÄ Training the cGAN\n",
    "\n",
    "The training process alternates between training the discriminator and the generator. The condition `y` (our attribute vector) is passed to both networks at each step.\n",
    "\n",
    "### Step 1: Train the Discriminator üïµÔ∏è‚Äç‚ôÇÔ∏è\n",
    "The discriminator's goal is to correctly classify real images that match their conditions, and reject fake images.\n",
    "$$ \\nabla_{\\theta_d} [\\log D(x|y) + \\log(1 - D(G(z|y)))] $$\n",
    "- We feed it a batch of **real images** with their **real attributes** and train it to classify them as `real` (label=1).\n",
    "- We then generate a batch of **fake images** using the generator (given the same real attributes) and train the discriminator to classify them as `fake` (label=0).\n",
    "\n",
    "### Step 2: Train the Generator üë®‚Äçüé®\n",
    "The generator's goal is to fool the discriminator into thinking its fake images are real and match the given attributes.\n",
    "$$ \\nabla_{\\theta_g} \\log(D(G(z|y))) $$\n",
    "- We generate a new batch of **fake images** with their corresponding attributes.\n",
    "- We pass them through the discriminator, but this time, we use `real` labels (label=1) for the loss calculation. This way, the backpropagated gradients update the **generator's weights** to produce images that the discriminator is more likely to classify as real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593263d8374a36c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fixed noise and condition vector to visualize generator progress\n",
    "sample_noise = torch.randn(10, Z).to(device)\n",
    "sample_conditions = torch.tensor([\n",
    "    [0, 0, 1, 0, 0],  # Young, Not Male\n",
    "    [1, 0, 1, 0, 0],  # Male, Young\n",
    "    [0, 1, 1, 0, 0],  # Smiling, Young\n",
    "    [1, 1, 0, 0, 0],  # Male, Smiling\n",
    "    [0, 0, 0, 1, 0],  # Eyeglasses\n",
    "    [1, 0, 0, 1, 0],  # Male, Eyeglasses\n",
    "    [0, 0, 0, 0, 1],  # Blond_Hair\n",
    "    [1, 0, 0, 0, 1],  # Male, Blond_Hair\n",
    "    [0, 1, 0, 0, 1],  # Smiling, Blond_Hair\n",
    "    [1, 1, 1, 1, 1],  # All attributes\n",
    "], dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d048e4e5736d09c",
   "metadata": {
    "id": "0WA5ea1jNG48"
   },
   "outputs": [],
   "source": [
    "# Lists to keep track of progress\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "discriminator.train()\n",
    "generator.train()\n",
    "\n",
    "# Training loop\n",
    "for i in range(1, EPOCHS+1):\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    for b, data in enumerate(pbar):\n",
    "        # Get the real images and their corresponding conditions (attributes)\n",
    "        inputs, conditions = data\n",
    "        inputs = inputs.to(device)\n",
    "        conditions = conditions.to(device)\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x|y)) + log(1 - D(G(z|y)))\n",
    "        ###########################\n",
    "\n",
    "        ## Train with all-real batch\n",
    "        discriminator.zero_grad()\n",
    "        label = torch.full((inputs.shape[0],), real_label, dtype=torch.float, device=device)\n",
    "        # Forward pass real batch through D\n",
    "        output = discriminator(inputs, conditions)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = loss_fn(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(inputs.shape[0], Z, device=device)\n",
    "        # Generate fake image batch with G, using the real conditions\n",
    "        fake = generator(noise, conditions)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D. We use .detach() to avoid backprop into generator here.\n",
    "        output = discriminator(fake.detach(), conditions)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = loss_fn(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Total discriminator loss is the sum of the real and fake losses\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizer_disc.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z|y)))\n",
    "        ###########################\n",
    "        generator.zero_grad()\n",
    "        label.fill_(real_label) # For the generator, we want the discriminator to think the fakes are real\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = discriminator(fake, conditions)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = loss_fn(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizer_gen.step()\n",
    "        \n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        pbar.set_description(f\"Epoch {i}/{EPOCHS}: \")\n",
    "        pbar.set_postfix({\n",
    "            \"G_loss\": f\"{errG.item():.3f}\", \n",
    "            \"D_loss\": f\"{errD.item():.3f}\", \n",
    "            \"D(x)\": f\"{D_x:.3f}\", \n",
    "            \"D(G(z))\": f\"{D_G_z2:.3f}\"\n",
    "        })\n",
    "        \n",
    "    # After each epoch, generate images with the fixed noise to see progress\n",
    "    with torch.no_grad():\n",
    "        generations = generator(sample_noise, sample_conditions).cpu()\n",
    "        # De-normalize from [-1,1] to [0,1] for display\n",
    "        generations = (generations + 1) / 2\n",
    "    display_image_grid(generations, sample_conditions.cpu(), 2, 5, f\"Generated images at epoch {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a1e8200d2916dc",
   "metadata": {},
   "source": [
    "### üìà Training Loss Visualization\n",
    "\n",
    "Plotting the losses helps us understand the training dynamics. Ideally, the two losses should reach an equilibrium, indicating that neither model is overpowering the other. \n",
    "- **Discriminator Loss (D)**: If this loss drops to zero, the generator is failing to produce convincing fakes, and training has stalled.\n",
    "- **Generator Loss (G)**: We want to see this loss stay competitive with the discriminator's. If it grows too high, the generator is being beaten too easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18693577a333721",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"Generator (G)\")\n",
    "plt.plot(D_losses,label=\"Discriminator (D)\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a454a243fe54ab",
   "metadata": {
    "id": "mDggVz4YBqIy"
   },
   "source": [
    "## ‚ú® Conditional Generation\n",
    "\n",
    "Now that training is complete, we can use our generator to create new, unique faces by providing it with random noise and our desired set of attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4c43a0a210f0bc",
   "metadata": {
    "id": "Tiw5cQfQLtFA"
   },
   "outputs": [],
   "source": [
    "# Generate a smaller grid first to test\n",
    "rows, cols = 2, 5  # Same as training\n",
    "sample_noise = torch.randn(rows*cols, Z).to(device)\n",
    "sample_conditions = torch.randint(0, 2, (rows*cols, NUM_ATTRIBUTES), dtype=torch.long).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generations = generator(sample_noise, sample_conditions).cpu()\n",
    "    generations = (generations + 1) / 2\n",
    "\n",
    "display_image_grid(generations, sample_conditions.cpu(), rows, cols, \"Final Conditionally Generated Faces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daa4806a090ff51",
   "metadata": {},
   "source": [
    "### ü§î Discussion\n",
    "\n",
    "The results show that the cGAN is capable of generating realistic faces that often reflect the requested attributes. For example, images conditioned on 'Blond_Hair' tend to have lighter hair, and those conditioned on 'Smiling' often feature a smile. The model isn't perfect, and sometimes the attributes are subtle or mixed, but the conditioning clearly influences the output.\n",
    "\n",
    "Challenges with cGANs are similar to standard GANs:\n",
    "- **Training Instability**: The balance between the generator and discriminator is still delicate.\n",
    "- **Mode Collapse**: The generator might still produce a limited variety of faces for a given condition.\n",
    "- **Attribute Entanglement**: The model might learn spurious correlations in the data (e.g., associating lipstick with being 'Female', even if not explicitly told to). The quality of the dataset's labels is crucial.\n",
    "\n",
    "Despite these challenges, cGANs are a powerful tool for controllable generation and form the basis for many advanced models like StyleGAN and image-to-image translation networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762d83aaf764337d",
   "metadata": {},
   "source": [
    "## üß™ What to Try Next\n",
    "\n",
    "- **Train for Longer**: Increasing the number of epochs can significantly improve the quality and coherence of the generated images.\n",
    "- **Explore Attribute Combinations**: Generate images with specific, interesting attribute combinations. What does a 'Male, Smiling, Blond_Hair, Eyeglasses' face look like according to the model?\n",
    "- **Latent Space Interpolation**: Create two noise vectors, `z1` and `z2`, and a fixed attribute vector `y`. Interpolate between `z1` and `z2` and see how the generated face changes while keeping the attributes constant.\n",
    "- **Attribute Interpolation**: Fix the noise vector `z` but interpolate between two different attribute vectors, `y1` and `y2`. For example, morph a non-smiling face into a smiling one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec4ee4f",
   "metadata": {},
   "source": [
    "### Contributed by: Ali Habibullah.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
