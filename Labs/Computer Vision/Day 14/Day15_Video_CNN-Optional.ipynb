{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5468b321",
   "metadata": {},
   "source": [
    "![image.png](https://i.imgur.com/a3uAqnb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1162c827",
   "metadata": {},
   "source": [
    "# Video Action Recognition with 3D-CNNs\n",
    "\n",
    "This notebook demonstrates how to build and train a 3D Convolutional Neural Network (3D-CNN) for video action recognition. Unlike standard 2D-CNNs that process static images, 3D-CNNs are designed to learn features from both spatial and temporal dimensions, making them ideal for understanding the content of videos.\n",
    "\n",
    "### **üìå The Core Idea: Learning from Space and Time**\n",
    "The model works by applying 3D convolution and pooling operations to a sequence of video frames.\n",
    "\n",
    "1.  **Input**: The model takes a short clip (a fixed number of frames) from a video as input.\n",
    "\n",
    "2.  **3D Convolutions**: Instead of a 2D kernel that slides over the height and width of an image, a 3D kernel slides over the **height, width, and time** (frames) of the video clip. This allows the network to learn motion patterns (like a person swinging a bat) in addition to visual features (the person, the bat).\n",
    "\n",
    "3.  **Classification**: After several layers of 3D convolutions and pooling, the learned features are passed through fully-connected layers to classify the action being performed in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ed9211486106f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tempfile\n",
    "import os\n",
    "import torchvision.io as io\n",
    "import torchvision.models.video as video_models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import Video, HTML\n",
    "from datasets import load_dataset\n",
    "from IPython.display import Video\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from torchvision import models\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020811d1",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ The Dataset: HMDB51\n",
    "\n",
    "We will use the **HMDB51 dataset**, a widely-used benchmark for action recognition. It consists of around 7,000 video clips categorized into 51 action classes.\n",
    "\n",
    "We'll load the dataset directly from the Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91617eb18efa45d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"NoahMartinezXiang/HMDB51\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74d6176",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Data Exploration and Visualization\n",
    "\n",
    "Before building the model, it's essential to understand and visualize our data. We'll define a helper function to play video clips directly within the notebook. This helps us get a feel for the different action classes and the nature of the video data (length, quality, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aec1d46f60850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depending on your environment, you may need to install specific libraries for video processing.\n",
    "# !pip install av opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d42fea2d18af52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a higher limit for the animation embedding size in Jupyter if needed\n",
    "# plt.rcParams['animation.embed_limit'] = 100\n",
    "\n",
    "def play_video_animation(video_data, label, fps=25):\n",
    "   \"\"\"Helper function to process and display a video from the dataset as an HTML animation.\"\"\"\n",
    "   frames = []\n",
    "\n",
    "   try:\n",
    "       # Iterate through the frames in the video data\n",
    "       for frame in video_data:\n",
    "           frame_data = frame['data']\n",
    "\n",
    "           # Convert tensor to numpy array\n",
    "           if hasattr(frame_data, 'numpy'):\n",
    "               frame_data = frame_data.numpy()\n",
    "           elif hasattr(frame_data, 'detach'):\n",
    "               frame_data = frame_data.detach().numpy()\n",
    "\n",
    "           # Ensure the channel order is H, W, C for matplotlib\n",
    "           if len(frame_data.shape) == 3 and frame_data.shape[0] == 3:\n",
    "               frame_data = np.transpose(frame_data, (1, 2, 0))\n",
    "\n",
    "           # Ensure data type is uint8 for display\n",
    "           if frame_data.dtype != np.uint8:\n",
    "               if frame_data.max() <= 1.0:\n",
    "                   frame_data = (frame_data * 255).astype(np.uint8)\n",
    "               else:\n",
    "                   frame_data = frame_data.astype(np.uint8)\n",
    "\n",
    "           frames.append(frame_data)\n",
    "   except:\n",
    "       # Some videos in the dataset might be corrupted\n",
    "       pass\n",
    "\n",
    "   if not frames:\n",
    "       print(f\"Could not load frames for video with label: {label}\")\n",
    "       return\n",
    "\n",
    "   fig, ax = plt.subplots(figsize=(8, 6))\n",
    "   ax.set_title(f'Video: {label} ({len(frames)} frames)')\n",
    "   ax.axis('off')\n",
    "\n",
    "   im = ax.imshow(frames[0])\n",
    "\n",
    "   # Animation function to update the frame\n",
    "   def animate(frame_idx):\n",
    "       im.set_array(frames[frame_idx])\n",
    "       ax.set_title(f'Video: {label} - Frame {frame_idx + 1}/{len(frames)}')\n",
    "       return [im]\n",
    "\n",
    "   anim = animation.FuncAnimation(\n",
    "       fig, animate, frames=len(frames),\n",
    "       interval=1000//fps, blit=False, repeat=True\n",
    "   )\n",
    "\n",
    "   plt.close()\n",
    "   return HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bad479ae8257a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display 5 random videos from the training set to see what they look like\n",
    "# plt.rcParams['animation.embed_limit'] = 100  # 100 MB limit, uncomment this if you want to see the full video\n",
    "\n",
    "random_indices = random.sample(range(len(ds['train'])), 5)\n",
    "label_names = ds['train'].features['label'].names\n",
    "\n",
    "for i, idx in enumerate(random_indices):\n",
    "   sample = ds['train'][idx]\n",
    "   video = sample['video']\n",
    "   label_idx = sample['label']\n",
    "   label_name = label_names[label_idx]\n",
    "\n",
    "   print(f\"Displaying random sample {i+1}: Action - {label_name}\")\n",
    "   display(play_video_animation(video, label_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f1c129",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ The PyTorch Dataset\n",
    "\n",
    "We create a custom `VideoDataset` class to interface with our data. This is the standard PyTorch way to prepare data for a `DataLoader`. Our dataset needs to perform several crucial preprocessing steps:\n",
    "\n",
    "1.  **Frame Sampling**: Videos have varying lengths. We need to feed a fixed-size input to our model, so we sample a constant `max_frames` from each video.\n",
    "2.  **Resizing**: Each frame is resized to a uniform `target_size` (e.g., 112x112) to ensure consistency.\n",
    "3.  **Normalization**: Frame pixel values are normalized to a common scale.\n",
    "4.  **Padding/Truncation**: If a video has fewer frames than `max_frames`, we pad it by repeating the last frame. If it has more, we truncate it.\n",
    "5.  **Tensor Conversion**: The final sequence of frames is converted into a single PyTorch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ff654f6452656a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None, max_frames=16, target_size=(112, 112)):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.max_frames = max_frames\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        video = sample['video']\n",
    "        label = sample['label']\n",
    "\n",
    "        frames = []\n",
    "        for frame in video:\n",
    "            # Extract the image data from the frame dictionary\n",
    "            frame_data = frame['data']\n",
    "\n",
    "            # Convert from PyTorch tensor to NumPy array if necessary\n",
    "            if hasattr(frame_data, 'numpy'):\n",
    "                frame_data = frame_data.numpy()\n",
    "            elif hasattr(frame_data, 'detach'):\n",
    "                frame_data = frame_data.detach().numpy()\n",
    "\n",
    "            # Transpose from (C, H, W) to (H, W, C) for resizing with OpenCV\n",
    "            if len(frame_data.shape) == 3 and frame_data.shape[0] == 3:\n",
    "                frame_data = np.transpose(frame_data, (1, 2, 0))\n",
    "\n",
    "            # Normalize pixel values to the 0-1 float range\n",
    "            if frame_data.dtype != np.float32:\n",
    "                if frame_data.max() > 1.0:\n",
    "                    frame_data = frame_data.astype(np.float32) / 255.0\n",
    "                else:\n",
    "                    frame_data = frame_data.astype(np.float32)\n",
    "\n",
    "            # Resize frame to the target spatial dimensions\n",
    "            frame_data = cv2.resize(frame_data, self.target_size)\n",
    "\n",
    "            # Transpose back to (C, H, W) format for PyTorch\n",
    "            if len(frame_data.shape) == 3:\n",
    "                frame_data = np.transpose(frame_data, (2, 0, 1))\n",
    "\n",
    "            frames.append(frame_data)\n",
    "\n",
    "            # Stop once we have reached the desired number of frames\n",
    "            if len(frames) >= self.max_frames:\n",
    "                break\n",
    "\n",
    "        # If the video is shorter than max_frames, pad by repeating the last frame\n",
    "        while len(frames) < self.max_frames:\n",
    "            # Handle the edge case of an empty video\n",
    "            frames.append(frames[-1] if frames else np.zeros((3, *self.target_size)))\n",
    "\n",
    "        # Ensure the final list has exactly max_frames\n",
    "        frames = frames[:self.max_frames]\n",
    "\n",
    "        # Stack the list of frames into a single tensor of shape (T, C, H, W)\n",
    "        video_tensor = torch.FloatTensor(np.array(frames))\n",
    "\n",
    "        # Apply any specified transformations (like normalization) to each frame\n",
    "        if self.transform:\n",
    "            transformed_frames = []\n",
    "            for frame in video_tensor:\n",
    "                transformed_frames.append(self.transform(frame))\n",
    "            video_tensor = torch.stack(transformed_frames)\n",
    "\n",
    "        return video_tensor, torch.LongTensor([label])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dca8ead",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ The Model: 3D Convolutional Neural Network (3D-CNN)\n",
    "\n",
    "Here we define the architecture of our 3D-CNN. It consists of a series of 3D convolutional blocks followed by fully-connected layers for classification.\n",
    "\n",
    "### Key Components:\n",
    "- **`nn.Conv3d`**: The core layer. It uses a 3D kernel to convolve over the video clip's height, width, and temporal (frame) dimensions simultaneously. This captures spatio-temporal features.\n",
    "- **`nn.BatchNorm3d`**: Normalizes the activations to stabilize training and improve performance.\n",
    "- **`nn.MaxPool3d`**: Downsamples the feature maps. Note the kernel sizes: `(1, 2, 2)` is used initially to reduce spatial dimensions while preserving the temporal information. Later, `(2, 2, 2)` is used to downsample in all three dimensions.\n",
    "- **`permute`**: In the `forward` method, the input tensor shape is changed from `(Batch, Time, Channels, Height, Width)` to `(Batch, Channels, Time, Height, Width)`, which is the format expected by PyTorch's `Conv3d` layers.\n",
    "- **`AdaptiveAvgPool3d`**: Reduces each feature map to a single value, making the model robust to different input sizes and preparing the features for the classifier.\n",
    "- **Fully-Connected Layers**: The final layers that perform the classification based on the extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74a611ab75d9817",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Video3DCNN(nn.Module):\n",
    "    def __init__(self, num_classes=51, input_channels=3):\n",
    "        super(Video3DCNN, self).__init__()\n",
    "        \n",
    "        # Block 1: 3D Conv -> BatchNorm -> ReLU -> MaxPool\n",
    "        self.conv3d1 = nn.Conv3d(input_channels, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.bn1 = nn.BatchNorm3d(64)\n",
    "        # Pool spatially but not temporally to preserve motion information early on\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "        \n",
    "        # Block 2\n",
    "        self.conv3d2 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.bn2 = nn.BatchNorm3d(128)\n",
    "        # Pool in all dimensions (time, height, width)\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "        \n",
    "        # Block 3\n",
    "        self.conv3d3 = nn.Conv3d(128, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.bn3 = nn.BatchNorm3d(256)\n",
    "        self.pool3 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "        \n",
    "        # Block 4\n",
    "        self.conv3d4 = nn.Conv3d(256, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.bn4 = nn.BatchNorm3d(512)\n",
    "        self.pool4 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "        \n",
    "        # Block 5\n",
    "        self.conv3d5 = nn.Conv3d(512, 512, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.bn5 = nn.BatchNorm3d(512)\n",
    "        self.pool5 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "        \n",
    "        # Global average pooling to reduce each feature map to a single value\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        \n",
    "        # Fully connected layers for classification\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, time, channels, height, width)\n",
    "        # Rearrange to (batch_size, channels, time, height, width) for 3D conv layers\n",
    "        x = x.permute(0, 2, 1, 3, 4)\n",
    "        \n",
    "        # Pass through convolutional blocks\n",
    "        x = F.relu(self.bn1(self.conv3d1(x)))\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.conv3d2(x)))\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = F.relu(self.bn3(self.conv3d3(x)))\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = F.relu(self.bn4(self.conv3d4(x)))\n",
    "        x = self.pool4(x)\n",
    "        \n",
    "        x = F.relu(self.bn5(self.conv3d5(x)))\n",
    "        x = self.pool5(x)\n",
    "        \n",
    "        # Apply global average pooling and flatten the output\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1) # Flatten to (batch_size, num_features)\n",
    "        \n",
    "        # Pass through fully connected layers with dropout for regularization\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccc9824",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Preparing for Training\n",
    "\n",
    "This section covers all the setup steps before we can start the training loop:\n",
    "1.  **Transformations**: We define a normalization transform using ImageNet statistics, a common practice.\n",
    "2.  **Data Splitting**: Training a model on the full HMDB51 dataset is computationally expensive. For this demonstration, we'll randomly sample a smaller subset of 2,000 videos and then split them into training and validation sets. We use stratified splitting to ensure both sets have a similar distribution of action classes.\n",
    "3.  **DataLoaders**: We create `DataLoader` instances for our training and validation sets, which will handle batching, shuffling, and multi-threaded data loading.\n",
    "4.  **Model Initialization**: We instantiate our `Video3DCNN`, set up the loss function (`CrossEntropyLoss`), optimizer (`Adam`), and a learning rate scheduler (`CosineAnnealingLR`) to adjust the learning rate during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8e30ce03f228d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define normalization transform using standard ImageNet stats\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665a4b153a21abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Randomly select a subset of samples from the dataset for faster training\n",
    "# Using 2000 samples for this demonstration\n",
    "total_samples = len(ds['train'])\n",
    "random_indices = random.sample(range(total_samples), min(1000, total_samples))\n",
    "\n",
    "# Split the selected indices into training and validation sets (80/20 split)\n",
    "# Stratification ensures that the class distribution is similar in both sets\n",
    "train_indices, val_indices = train_test_split(\n",
    "   random_indices,\n",
    "   test_size=0.2,\n",
    "   random_state=42,\n",
    "   stratify=[ds['train'][i]['label'] for i in random_indices]\n",
    ")\n",
    "\n",
    "# Create subset datasets from the indices\n",
    "train_subset = ds['train'].select(train_indices)\n",
    "val_subset = ds['train'].select(val_indices)\n",
    "\n",
    "print(f\"Training on {len(train_subset)} samples, validating on {len(val_subset)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439d3e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the VideoDataset instances for training and testing\n",
    "# We use 16 frames per video clip\n",
    "train_dataset = VideoDataset(train_subset, transform=transform, max_frames=16)\n",
    "test_dataset = VideoDataset(val_subset, transform=transform, max_frames=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef422335373f57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders to handle batching and shuffling\n",
    "batch_size = 16  # Small batch size due to memory constraints of 3D-CNNs\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fd39cc60491fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to a GPU if available, otherwise use the CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cee57bceef4579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and move it to the selected device\n",
    "model = Video3DCNN(num_classes=51)\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function for multi-class classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Adam optimizer with a small learning rate and weight decay for regularization\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler to gradually decrease the learning rate\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7d38212a408b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the total number of trainable parameters in the model\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7d1ecb",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Model Training\n",
    "\n",
    "We'll define two helper functions, `train_epoch` and `validate`, to encapsulate the logic for a single epoch of training and validation. This keeps the main training loop clean and organized.\n",
    "\n",
    "The training loop then iterates for a fixed number of `epochs`:\n",
    "1.  Calls `train_epoch` to train the model on the training data.\n",
    "2.  Calls `validate` to evaluate the model's performance on the unseen validation data.\n",
    "3.  Updates the learning rate using the scheduler.\n",
    "4.  Saves the model's weights if the validation accuracy improves, a technique known as **early stopping**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae105eac9a1ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Function to handle the training of the model for one epoch.\"\"\"\n",
    "    model.train() # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training')\n",
    "    for videos, labels in pbar:\n",
    "        # Move data to the target device\n",
    "        videos, labels = videos.to(device), labels.to(device)\n",
    "        \n",
    "        # Standard training steps\n",
    "        optimizer.zero_grad()       # Clear previous gradients\n",
    "        outputs = model(videos)     # Forward pass\n",
    "        loss = criterion(outputs, labels) # Calculate loss\n",
    "        loss.backward()             # Backpropagation\n",
    "        optimizer.step()            # Update weights\n",
    "        \n",
    "        # Track statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'Loss': f'{running_loss/len(pbar):.4f}',\n",
    "            'Acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    return running_loss / len(train_loader), 100. * correct / total\n",
    "\n",
    "def validate(model, test_loader, criterion, device):\n",
    "    \"\"\"Function to handle the validation of the model.\"\"\"\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad(): # Disable gradient calculations\n",
    "        pbar = tqdm(test_loader, desc='Validation')\n",
    "        for videos, labels in pbar:\n",
    "            videos, labels = videos.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(videos)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Track statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Store predictions and labels for classification report\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{running_loss/len(pbar):.4f}',\n",
    "                'Acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    return running_loss / len(test_loader), 100. * correct / total, all_predictions, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc3692f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store training and validation history for plotting\n",
    "train_losses, train_accs = [], []\n",
    "val_losses, val_accs = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7353a223e337e7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "best_acc = 0.0\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Train for one epoch\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # Validate the model\n",
    "    val_loss, val_acc, _, _ = validate(model, test_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    # Step the learning rate scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    # Save the model if it has the best validation accuracy so far\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_video_cnn.pth')\n",
    "        print(f\"New best model saved with accuracy: {best_acc:.2f}%\")\n",
    "\n",
    "print(f\"\\nTraining completed! Best validation accuracy: {best_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d86882",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Evaluating Performance\n",
    "\n",
    "After training, we evaluate the model's performance in two ways:\n",
    "\n",
    "1.  **Plotting Metrics**: We plot the training and validation loss and accuracy over epochs. This helps us visualize the learning process and check for signs of overfitting (e.g., if validation accuracy plateaus while training accuracy continues to rise).\n",
    "2.  **Classification Report**: We load the best-performing model (saved during training) and generate a detailed classification report. This report shows precision, recall, and F1-score for each action class, giving us a much deeper insight into the model's strengths and weaknesses than a single accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4c3e8f390778d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss and accuracy curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accs, label='Train Accuracy')\n",
    "plt.plot(val_accs, label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873c6c4baf6cc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model weights for final evaluation\n",
    "print(\"Loading best model for final evaluation...\")\n",
    "model.load_state_dict(torch.load('best_video_cnn.pth'))\n",
    "\n",
    "# Run validation on the test set to get final metrics and predictions\n",
    "val_loss, val_acc, predictions, true_labels = validate(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"\\nFinal Test Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "# Print a detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "label_names = ds['train'].features['label'].names\n",
    "print(classification_report(true_labels, predictions, target_names=label_names, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80cee9a",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Inference on New Samples\n",
    "\n",
    "Finally, let's see our trained model in action! We'll create a simple prediction function and use it to classify a few random videos from our validation set. This demonstrates how the model would be used in a real-world application to make predictions on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7769af2706cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_video(model, video_tensor, label_names, device):\n",
    "    \"\"\"Function to predict the class of a single video tensor.\"\"\"\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Add a batch dimension and move the tensor to the correct device\n",
    "        video_tensor = video_tensor.unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get model outputs (logits)\n",
    "        outputs = model(video_tensor)\n",
    "        \n",
    "        # Convert logits to probabilities using softmax\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        \n",
    "        # Get the index of the class with the highest probability\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Get the class name and confidence score\n",
    "        pred_class = label_names[predicted.item()]\n",
    "        confidence = probabilities[0][predicted].item() * 100\n",
    "\n",
    "        return pred_class, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ac4479550fde6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 5 random samples from the validation set for prediction\n",
    "random_test_indices = random.sample(range(len(test_dataset)), 5)\n",
    "for i, idx in enumerate(random_test_indices):\n",
    "    # Retrieve the preprocessed video tensor and its true label\n",
    "    video_tensor, true_label = test_dataset[idx]\n",
    "    true_class = label_names[true_label.item()]\n",
    "\n",
    "    # Make a prediction with our trained model\n",
    "    pred_class, confidence = predict_video(model, video_tensor, label_names, device)\n",
    "\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  True class: {true_class}\")\n",
    "    print(f\"  Predicted:  {pred_class} (confidence: {confidence:.1f}%)\")\n",
    "    print(f\"  Correct:    {'‚úì' if pred_class == true_class else '‚úó'}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57226c8b",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Conclusion & Next Steps\n",
    "\n",
    "We have successfully built, trained, and evaluated a 3D-CNN for video action recognition. Even with a relatively simple architecture and a small subset of data, the model learns to distinguish between different actions, demonstrating the power of learning from spatio-temporal features.\n",
    "\n",
    "### **üìù Exercises for Further Exploration**\n",
    "1.  **Train on More Data**: The most impactful change would be to train on the full HMDB51 dataset or for more `epochs`.\n",
    "2.  **Data Augmentation**: Implement video-specific data augmentation techniques, such as random horizontal flipping of the entire clip, random cropping in space and time, or color jittering.\n",
    "3.  **Use a Pre-trained Model**: Explore using a more advanced, pre-trained video model architecture like R(2+1)D (available in `torchvision.models.video`). You can fine-tune it on this dataset for potentially much better performance.\n",
    "       - I tried this, and got better results but took way lonegr to train, also I tried to use resnet as a backbone and it was similar results to the 3D cnn we have here. \n",
    "5.  **Experiment with Frame Sampling**: Instead of taking the first `N` frames, try a different sampling strategy. For example, you could sample `N` frames evenly spaced throughout the video.\n",
    "6.  **Hyperparameter Tuning**: Experiment with different values for `learning_rate`, `batch_size`, number of layers, or kernel sizes in the 3D-CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cb447c-f448-4c45-8ff8-4009d5d797d6",
   "metadata": {},
   "source": [
    "### Contributed by: Ali Habibullah"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
