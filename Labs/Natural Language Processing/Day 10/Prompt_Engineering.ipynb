{
 "cells": [
  {
   "metadata": {
    "id": "e3d690265dff2685"
   },
   "cell_type": "markdown",
   "source": [
    "![image.png](https://i.imgur.com/a3uAqnb.png)\n",
    "\n",
    "# Prompt Engineering: Mastering the Art of AI Communication\n",
    "\n",
    "This notebook demonstrates the fundamentals of **Prompt Engineering** - the practice of designing effective prompts to get better outputs from Large Language Models (LLMs). We'll explore different techniques that can dramatically improve model performance without any training or fine-tuning!\n",
    "\n",
    "### **üìå The Core Idea: Prompt Engineering**\n",
    "Prompt engineering is like learning to communicate effectively with an AI. Just as you might phrase a question differently for a child versus a professor, we need to craft our prompts to get the best responses from AI models.\n",
    "\n",
    "**What we'll cover:**\n",
    "1. **Basic Prompting**: Simple, direct instructions\n",
    "2. **Few-shot Learning**: Providing examples to guide the model\n",
    "3. **Chain-of-Thought (CoT)**: Teaching the model to reason step-by-step\n",
    "4. **Zero-shot CoT**: Getting reasoning without examples\n",
    "\n",
    "We'll use high-quality models that work well on 8GB VRAM: **Meta-Llama-3.2-3B-Instruct** for instruction following and **microsoft/Phi-3-mini-4k-instruct** for reasoning tasks - both are significantly more capable than older models!"
   ],
   "id": "e3d690265dff2685"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "id": "initial_id"
   },
   "source": [
    "# Install required packages (run this first)\n",
    "# %pip install torch torchvision torchaudio transformers accelerate bitsandbytes\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Suppress transformers warnings for clean output\n",
    "import logging\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "print(\"üìö Prompt Engineering Lab Setup Complete!\")\n",
    "print(f\"üîß Using device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "ad4dea58251f60bb"
   },
   "cell_type": "markdown",
   "source": [
    "## 1Ô∏è‚É£ Model Setup: High-Quality Models for 8GB VRAM\n",
    "\n",
    "For this lab, we'll use two state-of-the-art models optimized for performance:\n",
    "\n",
    "- **Meta-Llama-3.2-3B-Instruct**: A 3B parameter model with excellent instruction following\n",
    "- **Microsoft Phi-3-mini-4k-instruct**: A 3.8B parameter model optimized for reasoning\n",
    "\n",
    "Both models are:\n",
    "‚úÖ **High Quality**: Much better outputs than older models\n",
    "‚úÖ **8GB VRAM Compatible**: Using 4-bit quantization\n",
    "‚úÖ **Fast Inference**: Optimized for efficiency\n",
    "‚úÖ **Instruction Tuned**: Designed to follow prompts well\n",
    "\n",
    "These models represent the current state-of-the-art for efficient, high-quality language models!"
   ],
   "id": "ad4dea58251f60bb"
  },
  {
   "metadata": {
    "id": "ee75c2ddef692d61"
   },
   "cell_type": "code",
   "source": [
    "def setup_quantization():\n",
    "    \"\"\"Setup 4-bit quantization for memory efficiency\"\"\"\n",
    "    return BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "def load_models():\n",
    "    \"\"\"Load our high-quality prompt engineering models\"\"\"\n",
    "\n",
    "    quantization_config = setup_quantization() if torch.cuda.is_available() else None\n",
    "\n",
    "    print(\"üîÑ Loading Llama-3.2-3B-Instruct for instruction following...\")\n",
    "    # Llama 3.2 3B Instruct - excellent for instruction following\n",
    "    llama_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "    llama_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=\"eager\"\n",
    "    )\n",
    "\n",
    "    print(\"üîÑ Loading Phi-3-mini for reasoning tasks...\")\n",
    "    # Phi-3-mini - use older revision to avoid cache issues\n",
    "    phi_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "        revision=\"main\"\n",
    "    )\n",
    "    phi_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        trust_remote_code=True,\n",
    "        attn_implementation=\"eager\",\n",
    "        revision=\"main\",\n",
    "        use_cache=False  # Disable cache to avoid compatibility issues\n",
    "    )\n",
    "\n",
    "    # Set padding tokens\n",
    "    if llama_tokenizer.pad_token is None:\n",
    "        llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "    if phi_tokenizer.pad_token is None:\n",
    "        phi_tokenizer.pad_token = phi_tokenizer.eos_token\n",
    "\n",
    "    print(\"‚úÖ High-quality models loaded successfully!\")\n",
    "    return llama_tokenizer, llama_model, phi_tokenizer, phi_model\n",
    "\n",
    "# Load the models\n",
    "llama_tokenizer, llama_model, phi_tokenizer, phi_model = load_models()"
   ],
   "id": "ee75c2ddef692d61",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "afc91b471de67de7"
   },
   "cell_type": "code",
   "source": [
    "def generate_llama_response(prompt, max_new_tokens=150, temperature=0.7):\n",
    "    \"\"\"Generate response using Llama-3.2-3B-Instruct\"\"\"\n",
    "\n",
    "    # Format prompt for Llama instruction format\n",
    "    formatted_prompt = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "    inputs = llama_tokenizer(formatted_prompt, return_tensors=\"pt\", truncation=True)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = llama_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=llama_tokenizer.eos_token_id,\n",
    "            eos_token_id=llama_tokenizer.eos_token_id,\n",
    "            use_cache=False\n",
    "        )\n",
    "\n",
    "    # Decode only the new tokens (response)\n",
    "    response = llama_tokenizer.decode(outputs[0][len(inputs['input_ids'][0]):], skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "def generate_phi_response(prompt, max_new_tokens=150, temperature=0.7):\n",
    "    \"\"\"Generate response using Phi-3-mini\"\"\"\n",
    "\n",
    "    # Format prompt for Phi-3 instruction format\n",
    "    formatted_prompt = f\"<|user|>\\n{prompt}<|end|>\\n<|assistant|>\\n\"\n",
    "\n",
    "    inputs = phi_tokenizer(formatted_prompt, return_tensors=\"pt\", truncation=True)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = phi_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=phi_tokenizer.eos_token_id,\n",
    "            eos_token_id=phi_tokenizer.eos_token_id,\n",
    "            use_cache=False  # Disable cache to avoid compatibility issues\n",
    "        )\n",
    "\n",
    "    # Decode only the new tokens (response)\n",
    "    response = phi_tokenizer.decode(outputs[0][len(inputs['input_ids'][0]):], skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "def display_comparison(title, prompts_and_responses):\n",
    "    \"\"\"Display a nice comparison of different prompting approaches\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üéØ {title}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    for i, (prompt_type, prompt, response) in enumerate(prompts_and_responses, 1):\n",
    "        print(f\"\\nüìù Approach {i}: {prompt_type}\")\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Response: {response}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "print(\"üõ†Ô∏è Helper functions ready!\")"
   ],
   "id": "afc91b471de67de7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "d80587c8798d8f4b"
   },
   "cell_type": "markdown",
   "source": [
    "## 2Ô∏è‚É£ Basic Prompting: The Foundation\n",
    "\n",
    "Basic prompting is the simplest form of interaction with an LLM. You provide a direct instruction or question, and the model responds. The key is being **clear**, **specific**, and **concise**.\n",
    "\n",
    "### **üîπ Key Principles of Basic Prompting:**\n",
    "- **Be Specific**: Vague prompts lead to vague responses\n",
    "- **Provide Context**: Give the model enough information to understand your request\n",
    "- **Set Expectations**: Specify the format or type of response you want\n",
    "- **Use Clear Language**: Avoid ambiguity and complex sentence structures\n",
    "\n",
    "Let's see how different ways of asking the same question can yield very different results with our high-quality models!"
   ],
   "id": "d80587c8798d8f4b"
  },
  {
   "metadata": {
    "id": "28f0ac6bf424a0d8"
   },
   "cell_type": "code",
   "source": [
    "print(\"üöÄ Basic Prompting Examples\")\n",
    "\n",
    "# Example 1: Math Problem - Different levels of specificity\n",
    "math_prompts = [\n",
    "    (\"Vague\", \"Do some math\", \"\"),\n",
    "    (\"Basic\", \"What is 847 * 23?\", \"\"),\n",
    "    (\"Specific\", \"Calculate 847 multiplied by 23 and show your work step by step.\", \"\"),\n",
    "    (\"Very Specific\", \"Solve this multiplication problem step by step: 847 √ó 23 = ? Show each step of the calculation.\", \"\")\n",
    "]\n",
    "\n",
    "# Generate responses for math prompts using Phi-3 (better for math)\n",
    "for i, (prompt_type, prompt, _) in enumerate(math_prompts):\n",
    "    if prompt.strip() :\n",
    "        response = generate_phi_response(prompt)\n",
    "        math_prompts[i] = (prompt_type, prompt, response)\n",
    "\n",
    "# Filter out empty responses\n",
    "math_prompts = [(pt, p, r) for pt, p, r in math_prompts if r]\n",
    "\n",
    "display_comparison(\"Basic Prompting: Specificity Matters\", math_prompts)"
   ],
   "id": "28f0ac6bf424a0d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "db607a93fb3769a7"
   },
   "cell_type": "code",
   "source": [
    "# Example 2: Creative Writing - Context and Format Matter\n",
    "writing_prompts = [\n",
    "    (\"No Context\", \"Write a story.\", \"\"),\n",
    "    (\"Some Context\", \"Write a short story about a robot discovering emotions.\", \"\"),\n",
    "    (\"Clear Context & Format\", \"Write a 100-word story about a robot who discovers emotions when it finds an abandoned kitten. Include dialogue and end with a hopeful tone.\", \"\")\n",
    "]\n",
    "\n",
    "for i, (prompt_type, prompt, _) in enumerate(writing_prompts):\n",
    "    response = generate_llama_response(prompt, temperature=0.8)\n",
    "    writing_prompts[i] = (prompt_type, prompt, response)\n",
    "\n",
    "display_comparison(\"Basic Prompting: Context and Format\", writing_prompts)"
   ],
   "id": "db607a93fb3769a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "514fea91c6ac18d1"
   },
   "cell_type": "code",
   "source": [
    "# Example 3: Information Retrieval - Precision in Questions\n",
    "info_prompts = [\n",
    "    (\"Broad\", \"Tell me about climate change.\", \"\"),\n",
    "    (\"Focused\", \"What are the three main causes of climate change?\", \"\"),\n",
    "    (\"Targeted\", \"List the top 3 human activities that contribute most to greenhouse gas emissions, with a brief explanation for each.\", \"\")\n",
    "]\n",
    "\n",
    "for i, (prompt_type, prompt, _) in enumerate(info_prompts):\n",
    "    response = generate_llama_response(prompt)\n",
    "    info_prompts[i] = (prompt_type, prompt, response)\n",
    "\n",
    "display_comparison(\"Basic Prompting: Question Precision\", info_prompts)"
   ],
   "id": "514fea91c6ac18d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "8b779a7c398a1697"
   },
   "cell_type": "markdown",
   "source": [
    "## 3Ô∏è‚É£ Few-shot Learning: Learning by Example\n",
    "\n",
    "Few-shot learning is like showing someone examples before asking them to do a task. Instead of just giving instructions, we provide the model with **examples** of the input-output pattern we want it to follow.\n",
    "\n",
    "### **üîπ The Power of Examples:**\n",
    "- **Pattern Recognition**: The model learns the desired format and style\n",
    "- **Consistency**: Examples help ensure consistent output structure\n",
    "- **Complex Tasks**: Enables the model to handle tasks that are hard to describe in words\n",
    "- **Quality Control**: Examples set the standard for response quality\n",
    "\n",
    "### **üîπ Few-shot Structure:**\n",
    "- Example 1: [Input] ‚Üí [Expected Output]\n",
    "- Example 2: [Input] ‚Üí [Expected Output]\n",
    "- Example 3: [Input] ‚Üí [Expected Output]\n",
    "- Now do this: [Your actual input] ‚Üí [Model generates output]\n",
    "\n",
    "Our high-quality models excel at pattern recognition, making few-shot learning extremely effective!"
   ],
   "id": "8b779a7c398a1697"
  },
  {
   "metadata": {
    "id": "631351ec8d9e229d"
   },
   "cell_type": "code",
   "source": [
    "print(\"üéØ Few-shot Learning Examples\")\n",
    "\n",
    "# Example 1: Sentiment Analysis with Few-shot\n",
    "few_shot_sentiment_prompt = \"\"\"\n",
    "Review: \"This restaurant exceeded all my expectations! Amazing food and service.\"\n",
    "Sentiment: Positive\n",
    "\n",
    "Review: \"Terrible experience. Cold food, rude staff, overpriced.\"\n",
    "Sentiment: Negative\n",
    "\n",
    "Review: \"It was decent. Nothing special but not bad either.\"\n",
    "Sentiment: Neutral\n",
    "\n",
    "Classify the sentiment of the following review as Positive, Negative, or Neutral, using the previous examples:\n",
    "\n",
    "Review: \"The program barely works, but there is no alternative\"\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "# Compare with zero-shot\n",
    "zero_shot_sentiment_prompt = \"\"\"\n",
    "Classify the sentiment of this review as Positive, Negative, or Neutral:\n",
    "\n",
    "Review: \"The program barely works, but there is no alternative\"\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "sentiment_responses = [\n",
    "    (\"Zero-shot\", zero_shot_sentiment_prompt, \"\"),\n",
    "    (\"Few-shot\", few_shot_sentiment_prompt, \"\")\n",
    "]\n",
    "\n",
    "for i, (approach, prompt, _) in enumerate(sentiment_responses):\n",
    "    response = generate_llama_response(prompt, max_new_tokens=30)\n",
    "    sentiment_responses[i] = (approach, \"Software review sentiment\", response)\n",
    "\n",
    "display_comparison(\"Few-shot vs Zero-shot: Sentiment Analysis\", sentiment_responses)"
   ],
   "id": "631351ec8d9e229d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "d179f8eba8a3476d"
   },
   "cell_type": "code",
   "source": [
    "# Example 2: Code Documentation with Few-shot\n",
    "few_shot_code_prompt = \"\"\"\n",
    "Function: def add_numbers(a, b): return a + b\n",
    "Documentation: Adds two numbers together and returns the result. Parameters: a (int/float), b (int/float). Returns: sum of a and b.\n",
    "\n",
    "Function: def find_max(numbers): return max(numbers)\n",
    "Documentation: Finds the maximum value in a list of numbers. Parameters: numbers (list). Returns: maximum value from the list.\n",
    "\n",
    "Using the previous two examples as valid documentation, generate documentation for the following function:\n",
    "\n",
    "Function: def validate_email(email): return \"@\" in email and \".\" in email.split(\"@\")[1]\n",
    "\"\"\"\n",
    "\n",
    "zero_shot_code_prompt = \"\"\"\n",
    "Generate documentation for this Python function:\n",
    "\n",
    "Function: def validate_email(email): return \"@\" in email and \".\" in email.split(\"@\")[1]\n",
    "Documentation:\"\"\"\n",
    "\n",
    "code_responses = [\n",
    "    (\"Zero-shot\", zero_shot_code_prompt, \"\"),\n",
    "    (\"Few-shot\", few_shot_code_prompt, \"\")\n",
    "]\n",
    "\n",
    "for i, (approach, prompt, _) in enumerate(code_responses):\n",
    "    response = generate_phi_response(prompt, max_new_tokens=80)\n",
    "    code_responses[i] = (approach, \"Email validation function\", response)\n",
    "\n",
    "display_comparison(\"Few-shot vs Zero-shot: Code Documentation\", code_responses)"
   ],
   "id": "d179f8eba8a3476d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "9c0f58408c9e54af"
   },
   "cell_type": "code",
   "source": [
    "# Example 3: Complex Pattern - Data Analysis Summary\n",
    "few_shot_analysis_prompt = \"\"\"\n",
    "Dataset: Customer satisfaction survey (n=500, satisfaction score 4.2/5, 85% would recommend)\n",
    "Summary: HIGH SATISFACTION | Score: 4.2/5 | Recommendation rate: 85% | Sample: 500 customers | Action: Maintain current service quality\n",
    "\n",
    "Dataset: Website performance metrics (avg load time 2.1s, bounce rate 35%, conversion 3.2%)\n",
    "Summary: NEEDS IMPROVEMENT | Load time: 2.1s (slow) | Bounce rate: 35% (high) | Conversion: 3.2% | Action: Optimize page speed\n",
    "\n",
    "Using the previous two datasets as examples, please create a summary for the following dataset, use the same Summary format that was shown above.\n",
    "Dataset: Sales quarterly report (Q3 revenue $2.1M, 15% growth, target was $2M, top product: Software licenses)\n",
    "\"\"\"\n",
    "\n",
    "response = generate_llama_response(few_shot_analysis_prompt, max_new_tokens=80)\n",
    "print(f\"\\nüìä Few-shot Data Analysis Summary:\")\n",
    "print(f\"Input: Sales quarterly report with revenue, growth, and target data\")\n",
    "print(f\"Output: {response}\")"
   ],
   "id": "9c0f58408c9e54af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "2d64dd7f8dc30d3b"
   },
   "cell_type": "markdown",
   "source": [
    "## 4Ô∏è‚É£ Chain-of-Thought (CoT): Teaching the Model to Think\n",
    "\n",
    "Chain-of-Thought prompting is like asking someone to \"show their work\" on a complex problem. Instead of just getting the final answer, we guide the model to **think step-by-step** and show its reasoning process.\n",
    "\n",
    "### **üîπ Why CoT Works:**\n",
    "- **Complex Reasoning**: Breaks down difficult problems into manageable steps\n",
    "- **Improved Accuracy**: Step-by-step thinking reduces errors\n",
    "- **Transparency**: We can see how the model arrived at its conclusion\n",
    "- **Debugging**: If the answer is wrong, we can see where the reasoning failed\n",
    "\n",
    "### **üîπ CoT Structure:**\n",
    "**Problem:** [Complex question or task]\n",
    "**Let me think step by step:**\n",
    "\n",
    "1. [First reasoning step]\n",
    "2. [Second reasoning step]\n",
    "3. [Third reasoning step]\n",
    "\n",
    "...\n",
    "\n",
    "**Therefore,** [final answer]\n",
    "\n",
    "\n",
    "Our modern models have excellent reasoning capabilities and respond very well to CoT prompting!"
   ],
   "id": "2d64dd7f8dc30d3b"
  },
  {
   "metadata": {
    "id": "7dbdd8b9388e36e6"
   },
   "cell_type": "code",
   "source": [
    "print(\"üß† Chain-of-Thought Reasoning Examples\")\n",
    "\n",
    "# Example 1: Complex Math Word Problem\n",
    "direct_math_prompt = \"\"\"\n",
    "A company's revenue increased by 25% in Year 1, then decreased by 20% in Year 2. If they started with $800,000, what's their final revenue?\"\"\"\n",
    "\n",
    "cot_math_prompt = \"\"\"\n",
    "A company's revenue increased by 25% in Year 1, then decreased by 20% in Year 2. If they started with $800,000, what's their final revenue?\n",
    "\n",
    "Let me solve this step by step:\n",
    "1. Starting revenue: $800,000\n",
    "2. Year 1 increase of 25%: $800,000 √ó 1.25 = $1,000,000\n",
    "3. Year 2 decrease of 20%: $1,000,000 √ó 0.80 = $800,000\n",
    "Therefore, the final revenue is $800,000.\n",
    "\n",
    "Now solve this problem step by step:\n",
    "A store's profit margin was 15% in Q1, increased to 22% in Q2, then dropped to 18% in Q3. If Q1 sales were $500,000, and sales increased 10% each quarter, what was the profit in Q3?\n",
    "\n",
    "Let me solve this step by step:\"\"\"\n",
    "\n",
    "math_responses = [\n",
    "    (\"Direct\", \"A store's profit margin was 15% in Q1, increased to 22% in Q2, then dropped to 18% in Q3. If Q1 sales were $500,000, and sales increased 10% each quarter, what was the profit in Q3?\", \"\"),\n",
    "    (\"Chain-of-Thought\", cot_math_prompt, \"\")\n",
    "]\n",
    "\n",
    "for i, (approach, prompt, _) in enumerate(math_responses):\n",
    "    response = generate_phi_response(prompt, max_new_tokens=500)\n",
    "    math_responses[i] = (approach, \"Store profit calculation\", response)\n",
    "\n",
    "display_comparison(\"Chain-of-Thought: Complex Math Problem\", math_responses)"
   ],
   "id": "7dbdd8b9388e36e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "b6d2e1815ff9b976"
   },
   "cell_type": "code",
   "source": [
    "# Example 2: Logical Reasoning\n",
    "direct_logic_prompt = \"\"\"\n",
    "If \"No cats are dogs\" and \"All pets in this house are cats,\" what can we conclude about dogs in this house?\"\"\"\n",
    "\n",
    "cot_logic_prompt = \"\"\"\n",
    "If \"No cats are dogs\" and \"All pets in this house are cats,\" what can we conclude about dogs in this house?\n",
    "\n",
    "Let me analyze this step by step:\n",
    "1. Given: \"No cats are dogs\" - This means cats and dogs are mutually exclusive categories\n",
    "2. Given: \"All pets in this house are cats\" - Every pet in the house belongs to the cat category\n",
    "3. From (1): If something is a cat, it cannot be a dog\n",
    "4. From (2): All pets are cats\n",
    "5. Combining (3) and (4): Since all pets are cats, and no cats are dogs, no pets can be dogs\n",
    "Therefore, there can be no dogs among the pets in this house.\n",
    "\n",
    "Now analyze this logic problem step by step:\n",
    "If \"All successful entrepreneurs take risks\" and \"Maria is risk-averse,\" what can we conclude about Maria's entrepreneurial success?\n",
    "\n",
    "Let me analyze this step by step:\"\"\"\n",
    "\n",
    "logic_responses = [\n",
    "    (\"Direct\", \"If 'All successful entrepreneurs take risks' and 'Maria is risk-averse,' what can we conclude about Maria's entrepreneurial success?\", \"\"),\n",
    "    (\"Chain-of-Thought\", cot_logic_prompt, \"\")\n",
    "]\n",
    "\n",
    "for i, (approach, prompt, _) in enumerate(logic_responses):\n",
    "    response = generate_phi_response(prompt, max_new_tokens=100)\n",
    "    logic_responses[i] = (approach, \"Entrepreneurship logic problem\", response)\n",
    "\n",
    "display_comparison(\"Chain-of-Thought: Logical Reasoning\", logic_responses)"
   ],
   "id": "b6d2e1815ff9b976",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "2b6a80df24bdadf2"
   },
   "cell_type": "code",
   "source": [
    "# Example 3: Complex Decision Making with CoT\n",
    "decision_prompt = \"\"\"\n",
    "Should a small tech startup with 10 employees, $200K runway, and 6 months left accept a $50K investment offer that requires giving up 25% equity?\n",
    "\n",
    "Let me think through this decision step by step:\n",
    "1. Current situation analysis:\n",
    "   - Very limited runway (6 months)\n",
    "   - Small team size suggests early stage\n",
    "   - Need capital to survive and grow\n",
    "\n",
    "2. Investment offer evaluation:\n",
    "   - $50K extends runway by ~2-3 months (assuming $30K/month burn)\n",
    "   - 25% equity is significant for a small amount\n",
    "   - Valuation implied: $200K post-money\n",
    "\n",
    "3. Alternative considerations:\n",
    "   - Could they raise more money from other sources?\n",
    "   - Could they reduce burn rate instead?\n",
    "   - What's the growth trajectory?\n",
    "\n",
    "4. Risk assessment:\n",
    "   - Without funding: likely shutdown in 6 months\n",
    "   - With funding: more time but significant dilution\n",
    "   - Investor might provide valuable guidance\n",
    "\n",
    "Therefore, if no better alternatives exist, accepting the investment is likely the right choice to survive, despite the high dilution.\n",
    "\n",
    "Now help me think through this decision step by step:\n",
    "A freelance designer earning $80K/year is offered a full-time position at $70K/year plus benefits (health insurance worth $8K, 401k match worth $3K, paid vacation worth $5K). Should they take it?\n",
    "\n",
    "Let me think through this decision step by step:\"\"\"\n",
    "\n",
    "response = generate_llama_response(decision_prompt, max_new_tokens=400)\n",
    "print(f\"\\nü§î Chain-of-Thought Decision Making:\")\n",
    "print(f\"Problem: Freelancer considering full-time job offer\")\n",
    "print(f\"CoT Response: {response}\")"
   ],
   "id": "2b6a80df24bdadf2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "fbab0d88c0a4060c"
   },
   "cell_type": "markdown",
   "source": [
    "## 5Ô∏è‚É£ Zero-shot Chain-of-Thought: The Magic Phrase\n",
    "\n",
    "Zero-shot Chain-of-Thought is an incredibly simple yet powerful technique. By adding the phrase **\"Let's think step by step\"** to any prompt, we can often trigger step-by-step reasoning without providing any examples!\n",
    "\n",
    "### **üîπ The Magic of \"Let's think step by step\":**\n",
    "- **No Examples Needed**: Works without providing reasoning examples\n",
    "- **Universal Trigger**: Works across many different types of problems\n",
    "- **Emergent Behavior**: The model naturally breaks down complex problems\n",
    "- **Simple Implementation**: Just add one phrase to your existing prompts\n",
    "\n",
    "### **üîπ Zero-shot CoT Variations:**\n",
    "- \"Let's think step by step\"\n",
    "- \"Let's work through this systematically\"\n",
    "- \"Let me break this down step by step\"\n",
    "- \"Let's solve this step by step\"\n",
    "- \"Think about this carefully and systematically\"\n",
    "\n",
    "Our modern models respond exceptionally well to these reasoning triggers!"
   ],
   "id": "fbab0d88c0a4060c"
  },
  {
   "metadata": {
    "id": "17a234443885f50e"
   },
   "cell_type": "code",
   "source": [
    "print(\"‚ú® Zero-shot Chain-of-Thought: The Magic Phrase\")\n",
    "\n",
    "# Example 1: Complex Calculation - With and Without the Magic Phrase\n",
    "regular_prompt = \"A rectangular garden is 15 meters long and 8 meters wide. If you want to put a fence around it and fence costs $12 per meter, how much will the total cost be?\"\n",
    "\n",
    "zero_shot_cot_prompt = \"A rectangular garden is 15 meters long and 8 meters wide. If you want to put a fence around it and fence costs $12 per meter, how much will the total cost be? Let's think step by step.\"\n",
    "\n",
    "math_zero_shot = [\n",
    "    (\"Regular Prompt\", regular_prompt, \"\"),\n",
    "    (\"Zero-shot CoT\", zero_shot_cot_prompt, \"\")\n",
    "]\n",
    "\n",
    "for i, (approach, prompt, _) in enumerate(math_zero_shot):\n",
    "    response = generate_phi_response(prompt, max_new_tokens=100)\n",
    "    math_zero_shot[i] = (approach, \"Garden fence problem\", response)\n",
    "\n",
    "display_comparison(\"Zero-shot CoT: Math Problem\", math_zero_shot)"
   ],
   "id": "17a234443885f50e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "36c07eb557327cfd"
   },
   "cell_type": "code",
   "source": [
    "# Example 2: Strategy Problem\n",
    "regular_strategy_prompt = \"A new social media app has 1000 users after 3 months. Competitors have millions. What should they focus on to grow?\"\n",
    "\n",
    "zero_shot_strategy_prompt = \"A new social media app has 1000 users after 3 months. Competitors have millions. What should they focus on to grow? Let's think step by step.\"\n",
    "\n",
    "strategy_zero_shot = [\n",
    "    (\"Regular Prompt\", regular_strategy_prompt, \"\"),\n",
    "    (\"Zero-shot CoT\", zero_shot_strategy_prompt, \"\")\n",
    "]\n",
    "\n",
    "for i, (approach, prompt, _) in enumerate(strategy_zero_shot):\n",
    "    response = generate_llama_response(prompt, max_new_tokens=200)\n",
    "    strategy_zero_shot[i] = (approach, \"App growth strategy\", response)\n",
    "\n",
    "display_comparison(\"Zero-shot CoT: Strategy Problem\", strategy_zero_shot)\n"
   ],
   "id": "36c07eb557327cfd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "9c971f83b32ff06"
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Example 3: Technical Debugging\n",
    "regular_debug_prompt = \"My Python web app is running slowly. Users complain about 5-second load times. How should I troubleshoot this?\"\n",
    "\n",
    "zero_shot_debug_prompt = \"My Python web app is running slowly. Users complain about 5-second load times. How should I troubleshoot this? Let's think step by step.\"\n",
    "\n",
    "debug_zero_shot = [\n",
    "    (\"Regular Prompt\", regular_debug_prompt, \"\"),\n",
    "    (\"Zero-shot CoT\", zero_shot_debug_prompt, \"\")\n",
    "]\n",
    "\n",
    "for i, (approach, prompt, _) in enumerate(debug_zero_shot):\n",
    "    response = generate_phi_response(prompt, max_new_tokens=300)\n",
    "    debug_zero_shot[i] = (approach, \"Performance debugging\", response)\n",
    "\n",
    "display_comparison(\"Zero-shot CoT: Technical Problem\", debug_zero_shot)"
   ],
   "id": "9c971f83b32ff06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "6696ec8c37d40d32"
   },
   "cell_type": "markdown",
   "source": [
    "## 6Ô∏è‚É£ Advanced Prompt Engineering Techniques\n",
    "\n",
    "Now that we've mastered the basics, let's explore some advanced techniques that can further improve your prompt engineering skills:\n",
    "\n",
    "### **üîπ Role-Playing**: Having the AI adopt specific personas or expertise\n",
    "### **üîπ Output Formatting**: Controlling the structure and format of responses\n",
    "### **üîπ Constraint Setting**: Adding specific limitations or requirements\n",
    "### **üîπ Multi-step Prompting**: Breaking complex tasks into smaller steps\n",
    "### **üîπ Self-Correction**: Having the model check and improve its own work"
   ],
   "id": "6696ec8c37d40d32"
  },
  {
   "metadata": {
    "id": "cea474d3e316f7c3"
   },
   "cell_type": "code",
   "source": [
    "print(\"üöÄ Advanced Prompt Engineering Techniques\")\n",
    "\n",
    "# Technique 1: Expert Role-Playing\n",
    "role_playing_prompt = \"\"\"\n",
    "You are a senior cybersecurity expert with 15 years of experience in enterprise security.\n",
    "A small business owner asks: \"I have 20 employees using personal devices for work. What are the top 3 security risks I should address immediately?\"\n",
    "\n",
    "Provide specific, actionable advice with your expert perspective:\"\"\"\n",
    "\n",
    "response = generate_llama_response(role_playing_prompt, max_new_tokens=400)\n",
    "print(f\"\\nüíº Expert Role-Playing Technique:\")\n",
    "print(f\"Response: {response}\")\n"
   ],
   "id": "cea474d3e316f7c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "8f848bb8ba579a67"
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Technique 2: Structured Output Formatting\n",
    "formatting_prompt = \"\"\"\n",
    "Analyze the pros and cons of electric vehicles vs gasoline cars. Format your response exactly as:\n",
    "\n",
    "ELECTRIC VEHICLES:\n",
    "Advantages:\n",
    "- [advantage 1 with brief explanation]\n",
    "- [advantage 2 with brief explanation]\n",
    "- [advantage 3 with brief explanation]\n",
    "Disadvantages:\n",
    "- [disadvantage 1 with brief explanation]\n",
    "- [disadvantage 2 with brief explanation]\n",
    "\n",
    "GASOLINE CARS:\n",
    "Advantages:\n",
    "- [advantage 1 with brief explanation]\n",
    "- [advantage 2 with brief explanation]\n",
    "- [advantage 3 with brief explanation]\n",
    "Disadvantages:\n",
    "- [disadvantage 1 with brief explanation]\n",
    "- [disadvantage 2 with brief explanation]\"\"\"\n",
    "\n",
    "response = generate_llama_response(formatting_prompt, max_new_tokens=500)\n",
    "print(f\"\\nüìã Structured Output Formatting:\")\n",
    "print(f\"Response: {response}\")\n"
   ],
   "id": "8f848bb8ba579a67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "a0a103245fc0306a"
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Technique 3: Self-Correction\n",
    "self_correction_prompt = \"\"\"\n",
    "Solve this problem: \"If a train travels 240 km in 3 hours, what's its average speed in mph?\"\n",
    "\n",
    "First, provide your initial answer. Then, check your work and identify any errors. Finally, provide the corrected answer if needed.\n",
    "\n",
    "Initial solution:\"\"\"\n",
    "\n",
    "response = generate_phi_response(self_correction_prompt, max_new_tokens=1000)\n",
    "print(f\"\\nüîç Self-Correction Technique:\")\n",
    "print(f\"Response: {response}\")"
   ],
   "id": "a0a103245fc0306a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "a4131b93d241adab"
   },
   "cell_type": "code",
   "source": [
    "# Technique 4: Multi-step Complex Problem Solving\n",
    "complex_prompt = \"\"\"\n",
    "You're helping a startup founder make a critical decision. They have these options:\n",
    "\n",
    "Option A: Raise $500K VC funding (30% equity, 18 months runway)\n",
    "Option B: Take $100K angel investment (8% equity, 6 months runway)\n",
    "Option C: Bootstrap with current $50K savings (3 months runway)\n",
    "\n",
    "Additional context: B2B SaaS product, 2 co-founders, early traction (10 paying customers, $2K MRR), growing 20% monthly.\n",
    "\n",
    "Please analyze this systematically:\n",
    "\n",
    "Step 1: Evaluate each option's financial implications\n",
    "Step 2: Assess the strategic value and risks\n",
    "Step 3: Consider timeline and growth requirements\n",
    "Step 4: Make a recommendation with reasoning\n",
    "\n",
    "Step 1 - Financial Analysis:\"\"\"\n",
    "\n",
    "response = generate_llama_response(complex_prompt, max_new_tokens=1500)\n",
    "print(f\"\\nüéØ Multi-step Complex Problem Solving:\")\n",
    "print(f\"Response: {response}\")"
   ],
   "id": "a4131b93d241adab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "3089e083207a47b8"
   },
   "cell_type": "markdown",
   "source": [
    "## 7Ô∏è‚É£ Prompt Engineering Best Practices & Tips\n",
    "\n",
    "### **üéØ The Golden Rules of Prompt Engineering:**\n",
    "\n",
    "#### **1. Clarity is King**\n",
    "- Use simple, clear language\n",
    "- Avoid ambiguous terms\n",
    "- Be specific about what you want\n",
    "\n",
    "#### **2. Context is Crucial**\n",
    "- Provide relevant background information\n",
    "- Set the scene for your request\n",
    "- Include constraints and requirements\n",
    "- How to get the context you may ask?\n",
    "![RAGS](https://m.media-amazon.com/images/I/91k5FXf3d6L.jpg)\n",
    "\n",
    "#### **3. Examples Are Powerful**\n",
    "- Show, don't just tell\n",
    "- Use diverse examples\n",
    "- Include edge cases when relevant\n",
    "\n",
    "#### **4. Structure Matters**\n",
    "- Use clear formatting and organization\n",
    "- Break complex tasks into steps\n",
    "- Specify desired output format\n",
    "\n",
    "#### **5. Iterate and Refine**\n",
    "- Start simple, then add complexity\n",
    "- Test variations of your prompts\n",
    "- Learn from what works and what doesn't\n",
    "\n",
    "#### **6. Know Your Model**\n",
    "- Modern models like Llama-3.2 and Phi-3 are highly capable\n",
    "- They understand context and nuance well\n",
    "- They respond excellently to reasoning prompts\n",
    "\n",
    "### **‚ö° Pro Tips for Better Results:**\n",
    "- Use \"Let's think step by step\" for complex problems\n",
    "- Add \"You are an expert in...\" for specialized knowledge\n",
    "- Include \"Be specific and detailed\" for comprehensive answers\n",
    "- Use \"Format your response as...\" for structured output\n",
    "- Try \"First..., then..., finally...\" for multi-step tasks\n",
    "\n",
    "### **üö´ Common Mistakes to Avoid:**\n",
    "- Being too vague in your requests\n",
    "- Not providing enough context\n",
    "- Asking multiple unrelated questions in one prompt\n",
    "- Forgetting to specify the desired output format\n",
    "- Not testing your prompts with variations"
   ],
   "id": "3089e083207a47b8"
  },
  {
   "metadata": {
    "id": "dc94b55865ae0db9"
   },
   "cell_type": "code",
   "source": [
    "print(\"üéì Practical Exercise: Design Your Own Prompts\")\n",
    "\n",
    "# Exercise: Create different prompts for the same task\n",
    "task = \"Help someone create a comprehensive business plan for a food truck\"\n",
    "prompts_to_test = [\n",
    "\n",
    "\n",
    "]\n",
    "# Design prompts using different techniques!\n",
    "# Example prompts:\n",
    "# prompts_to_test = [\n",
    "#     (\"Basic\", \"How do I write a business plan for a food truck?\"),\n",
    "#\n",
    "#     (\"Specific + Context\", \"I want to start a gourmet burger food truck in Austin, Texas with $80,000 startup capital. Create a comprehensive business plan covering market analysis, financial projections, and operations.\"),\n",
    "#\n",
    "#     (\"Expert Role-playing\", \"You are a successful food truck entrepreneur and business consultant with 10 years of experience. Help me create a detailed business plan for a gourmet burger food truck in Austin, Texas with $80,000 startup capital. Include specific insights from your experience.\"),\n",
    "#\n",
    "#     (\"Zero-shot CoT\", \"I need a business plan for a gourmet burger food truck in Austin, Texas with $80,000 startup capital. Let's think step by step about all the components I need to address.\"),\n",
    "#      ]\n",
    "\n",
    "print(\"\\nüîç Testing Different Prompt Approaches for Business Plan:\")\n",
    "for approach, prompt in prompts_to_test:\n",
    "    response = generate_llama_response(prompt, max_new_tokens=150)\n",
    "    print(f\"\\nüìù {approach}:\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 50)"
   ],
   "id": "dc94b55865ae0db9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "cbcbf6351449cf9d"
   },
   "cell_type": "markdown",
   "source": [
    "# In conclusion\n",
    "\n",
    "![Prompting](https://i.imgur.com/iaEypZu.png)"
   ],
   "id": "cbcbf6351449cf9d"
  },
  {
   "metadata": {
    "id": "18ffb4ee7408268e"
   },
   "cell_type": "markdown",
   "source": [
    "### Contributed by: Ali Habibullah\n",
    "\n"
   ],
   "id": "18ffb4ee7408268e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
