{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80f7860d50c399ef",
   "metadata": {},
   "source": [
    "![image.png](https://i.imgur.com/a3uAqnb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c851d31d6ef79",
   "metadata": {},
   "source": [
    "# Arabic-English Seq2Seq Translation Model\n",
    "\n",
    "![Neural Machine Translation](https://miro.medium.com/v2/resize:fit:1400/1*sO-SP58T4brE9EHazHSeGA.png)\n",
    "\n",
    "## **‚ö†Ô∏è Important Note on Training Approach**\n",
    "\n",
    "**This notebook demonstrates Seq2Seq architecture for Arabic-English translation, but uses a simplified training approach for educational purposes:**\n",
    "\n",
    "- ‚úÖ **Purpose**: Showcase Seq2Seq implementation and Arabic NLP challenges\n",
    "- ‚ö†Ô∏è **Limitation**: Only uses training set (no validation/test split)\n",
    "- üéØ **Result**: Model will overfit, but this demonstrates the complexity of Arabic translation\n",
    "- üìö **Learning Goal**: Understand architecture, not production-ready model\n",
    "\n",
    "**In production, you would need proper train/validation/test splits and regularization techniques!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cc8960257f1373",
   "metadata": {},
   "source": [
    "\n",
    "## **üìå Sequence-to-Sequence (Seq2Seq) Model**\n",
    "\n",
    "A **Seq2Seq model** consists of two main components:\n",
    "1. **Encoder**: Processes input sequence (Arabic) into fixed-size context vector\n",
    "2. **Decoder**: Generates output sequence (English) from context vector\n",
    "\n",
    "### **üîπ Key Features**\n",
    "- **Variable-length inputs/outputs**: Can handle sequences of different lengths\n",
    "- **Attention mechanism**: (Not implemented here, but commonly used)\n",
    "- **Teacher forcing**: Uses ground truth during training for faster convergence\n",
    "\n",
    "### **üîπ Architecture Overview**\n",
    "```\n",
    "Arabic Text ‚Üí Tokenize ‚Üí Encoder LSTM ‚Üí Context Vector ‚Üí Decoder LSTM ‚Üí English Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a032705b51f582",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Dataset Loading and Initial Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70245a1961783267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Load the Arabic-English translation dataset\n",
    "dataset = load_dataset(\"Abdulmohsena/Classic-Arabic-English-Language-Pairs\")\n",
    "print(f\"Dataset loaded: {len(dataset['books'])} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c648578cff507416",
   "metadata": {},
   "source": [
    "**üìù What's happening here:**\n",
    "- Loading a parallel Arabic-English corpus\n",
    "- This dataset contains classical Arabic texts with English translations\n",
    "- Arabic is a morphologically rich language with complex grammar rules\n",
    "- ŸÜÿµŸÜÿß ŸÖÿß Ÿäÿπÿ±ŸÅ ÿßŸÑŸÜÿ≠Ÿà ÿßŸÑÿπÿ±ÿ®Ÿä"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4b75ae15dca35a",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Vocabulary Analysis and Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcd61128ea4684d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize(text):\n",
    "    \"\"\"\n",
    "    Simple tokenization function that:\n",
    "    1. Removes punctuation\n",
    "    2. Splits on whitespace\n",
    "    Note: For Arabic, proper tokenization is more complex!\n",
    "    \"\"\"\n",
    "    cleaned = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    return cleaned.split()\n",
    "\n",
    "# Build vocabularies\n",
    "arabic_words = []\n",
    "english_words = []\n",
    "for text in dataset['books']['ar']:\n",
    "    arabic_words.extend(simple_tokenize(text))\n",
    "for text in dataset['books']['en']:\n",
    "    english_words.extend(simple_tokenize(text.lower()))\n",
    "\n",
    "arabic_vocab = Counter(arabic_words)\n",
    "english_vocab = Counter(english_words)\n",
    "print(f\"Arabic vocab: {len(arabic_vocab):,} unique words\")\n",
    "print(f\"English vocab: {len(english_vocab):,} unique words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fecee3293c3a5c6",
   "metadata": {},
   "source": [
    "**üìù Key Observations:**\n",
    "- Arabic has ~4x more unique words than English due to morphological complexity\n",
    "- Arabic words have roots with various prefixes/suffixes\n",
    "- This simple tokenization doesn't handle Arabic morphology properly\n",
    "\n",
    "**üîπ Arabic NLP Challenges:**\n",
    "1. **Right-to-left script**\n",
    "2. **No capitalization**\n",
    "3. **Rich morphology** (one root ‚Üí many word forms)\n",
    "4. **Diacritics** (optional vowel markings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8977bbce4f57098",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Vocabulary Reduction and Mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd6135e1293ceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "ARABIC_VOCAB_SIZE = 8000\n",
    "ENGLISH_VOCAB_SIZE = 5000\n",
    "SPECIAL_TOKENS = ['<PAD>', '< SOS >', '<EOS>', '<UNK>']\n",
    "\n",
    "def create_vocab_mapping(vocab_dict, special_tokens):\n",
    "    \"\"\"\n",
    "    Creates bidirectional mappings between words and indices.\n",
    "    \n",
    "    Special tokens:\n",
    "    - <PAD>: Padding for variable-length sequences\n",
    "    - <SOS>: Start of sequence marker\n",
    "    - <EOS>: End of sequence marker  \n",
    "    - <UNK>: Unknown words (out of vocabulary)\n",
    "    \"\"\"\n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    \n",
    "    # Add special tokens first (indices 0-3)\n",
    "    for i, token in enumerate(special_tokens):\n",
    "        word2idx[token] = i\n",
    "        idx2word[i] = token\n",
    "    \n",
    "    # Add most frequent words\n",
    "    for i, word in enumerate(vocab_dict.keys(), len(special_tokens)):\n",
    "        word2idx[word] = i\n",
    "        idx2word[i] = word\n",
    "    \n",
    "    return word2idx, idx2word\n",
    "\n",
    "# Create reduced vocabularies (keep only most frequent words)\n",
    "arabic_vocab_reduced = dict(arabic_vocab.most_common(ARABIC_VOCAB_SIZE))\n",
    "english_vocab_reduced = dict(english_vocab.most_common(ENGLISH_VOCAB_SIZE))\n",
    "\n",
    "ar_word2idx, ar_idx2word = create_vocab_mapping(arabic_vocab_reduced, SPECIAL_TOKENS)\n",
    "en_word2idx, en_idx2word = create_vocab_mapping(english_vocab_reduced, SPECIAL_TOKENS)\n",
    "\n",
    "print(f\"Reduced vocabularies: Arabic={len(ar_word2idx)}, English={len(en_word2idx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1672fda03a3c5795",
   "metadata": {},
   "source": [
    "**üìù Why vocabulary reduction?**\n",
    "- **Memory efficiency**: Smaller embedding tables\n",
    "- **Training speed**: Fewer parameters to update\n",
    "- **Generalization**: Focus on most important words\n",
    "- **Trade-off**: Some words become `<UNK>` tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b703981ddc3f06",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Data Preprocessing and Sequence Conversion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649117367d58c897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    \"\"\"\n",
    "    Clean and tokenize text:\n",
    "    1. Remove bracketed annotations [...]\n",
    "    2. Remove punctuation\n",
    "    3. Convert to lowercase\n",
    "    4. Split into tokens\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # Remove annotations\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Remove punctuation\n",
    "    text = text.strip().lower() if text.strip() else ''\n",
    "    return text.split() if text else []\n",
    "\n",
    "def text_to_sequence(text, word2idx, max_length=None):\n",
    "    \"\"\"\n",
    "    Convert text to sequence of token indices:\n",
    "    1. Add <SOS> token at start\n",
    "    2. Convert words to indices (use <UNK> for unknown words)\n",
    "    3. Add <EOS> token at end\n",
    "    4. Truncate if exceeds max_length\n",
    "    \"\"\"\n",
    "    tokens = clean_and_tokenize(text)\n",
    "    sequence = [word2idx.get('< SOS >', 1)]  # Start token\n",
    "    \n",
    "    for token in tokens:\n",
    "        idx = word2idx.get(token, word2idx.get('<UNK>', 3))\n",
    "        sequence.append(idx)\n",
    "    \n",
    "    sequence.append(word2idx.get('<EOS>', 2))  # End token\n",
    "    \n",
    "    # Truncate if too long\n",
    "    if max_length and len(sequence) > max_length:\n",
    "        sequence = sequence[:max_length-1] + [word2idx.get('<EOS>', 2)]\n",
    "    \n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8b7b405ae9f6cd",
   "metadata": {},
   "source": [
    "**üìù Sequence format:**\n",
    "```\n",
    "Original: \"ŸÖÿ±ÿ≠ÿ®ÿß ÿ®ŸÉŸÖ\"\n",
    "Tokenized: [\"ŸÖÿ±ÿ≠ÿ®ÿß\", \"ÿ®ŸÉŸÖ\"]\n",
    "Sequence: [<SOS>, ŸÖÿ±ÿ≠ÿ®ÿß_idx, ÿ®ŸÉŸÖ_idx, <EOS>]\n",
    "Indices: [1, 145, 892, 2]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f418e498749bb37",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Dataset Creation and DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5463aed72d9a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSET_SIZE = 5000  # üö® Using subset for quick training (NOT recommended for production!)\n",
    "\n",
    "subset_data = {\n",
    "    'ar': dataset['books']['ar'][:SUBSET_SIZE],\n",
    "    'en': dataset['books']['en'][:SUBSET_SIZE]\n",
    "}\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for Arabic-English translation pairs.\n",
    "    Filters out sequences that are too short/long for stable training.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, ar_word2idx, en_word2idx, max_ar_len=60, max_en_len=100):\n",
    "        self.samples = []\n",
    "        \n",
    "        for i in range(len(data['ar'])):\n",
    "            ar_seq = text_to_sequence(data['ar'][i], ar_word2idx, max_ar_len)\n",
    "            en_seq = text_to_sequence(data['en'][i], en_word2idx, max_en_len)\n",
    "            \n",
    "            # Filter: keep sequences with reasonable length\n",
    "            if 3 <= len(ar_seq) <= max_ar_len and 3 <= len(en_seq) <= max_en_len:\n",
    "                self.samples.append((ar_seq, en_seq))\n",
    "        \n",
    "        print(f\"Dataset: {len(data['ar'])} ‚Üí {len(self.samples)} samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ar_seq, en_seq = self.samples[idx]\n",
    "        return {\n",
    "            'arabic': torch.tensor(ar_seq, dtype=torch.long),\n",
    "            'english': torch.tensor(en_seq, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable-length sequences.\n",
    "    Pads sequences to the same length within each batch.\n",
    "    \"\"\"\n",
    "    arabic_seqs = [item['arabic'] for item in batch]\n",
    "    english_seqs = [item['english'] for item in batch]\n",
    "    \n",
    "    # Pad sequences (padding_value=0 corresponds to <PAD> token)\n",
    "    arabic_padded = pad_sequence(arabic_seqs, batch_first=True, padding_value=0)\n",
    "    english_padded = pad_sequence(english_seqs, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return {'arabic': arabic_padded, 'english': english_padded}\n",
    "\n",
    "train_dataset = TranslationDataset(subset_data, ar_word2idx, en_word2idx)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95df532bc947a7c",
   "metadata": {},
   "source": [
    "**üö® Training Data Issue:**\n",
    "- We're using the same data for training that we'll test on\n",
    "- This will lead to **severe overfitting**\n",
    "- Model will memorize rather than generalize\n",
    "- **For demonstration purposes only!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7add95988112b34",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Seq2Seq Model Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6898e6654ce45fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based encoder that processes Arabic input sequence.\n",
    "    \n",
    "    Flow: Arabic tokens ‚Üí Embeddings ‚Üí LSTM ‚Üí Hidden states\n",
    "    Returns final hidden and cell states as context for decoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, \n",
    "                           batch_first=True, dropout=0.3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embed_size)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        # hidden, cell: (num_layers, batch_size, hidden_size)\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based decoder that generates English output sequence.\n",
    "    \n",
    "    Uses teacher forcing during training:\n",
    "    - Input: previous ground truth token\n",
    "    - Output: prediction for next token\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, \n",
    "                           batch_first=True, dropout=0.3)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)  # Project to vocab size\n",
    "        \n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x shape: (batch_size, 1) - single token input\n",
    "        embedded = self.embedding(x)  # (batch_size, 1, embed_size)\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        # output: (batch_size, 1, hidden_size)\n",
    "        prediction = self.fc(output)  # (batch_size, 1, vocab_size)\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Sequence-to-Sequence model combining encoder and decoder.\n",
    "    \n",
    "    Training process:\n",
    "    1. Encode Arabic sequence to context vector\n",
    "    2. Decode context to English sequence using teacher forcing\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        \"\"\"\n",
    "        Forward pass with teacher forcing.\n",
    "        \n",
    "        Args:\n",
    "            src: Arabic input sequence (batch_size, src_len)\n",
    "            trg: English target sequence (batch_size, trg_len)\n",
    "            teacher_forcing_ratio: Probability of using ground truth vs model prediction\n",
    "        \"\"\"\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.vocab_size\n",
    "        \n",
    "        # Store all predictions\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        # Encode source sequence\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        # First decoder input is <SOS> token\n",
    "        input_token = trg[:, 0].unsqueeze(1)  # (batch_size, 1)\n",
    "        \n",
    "        # Generate sequence token by token\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input_token, hidden, cell)\n",
    "            outputs[:, t] = output.squeeze(1)\n",
    "            \n",
    "            # Teacher forcing: use ground truth or model prediction\n",
    "            use_teacher_forcing = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(2)  # Get predicted token\n",
    "            input_token = trg[:, t].unsqueeze(1) if use_teacher_forcing else top1\n",
    "            \n",
    "        return outputs\n",
    "\n",
    "# Create model components\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "encoder = Encoder(len(ar_word2idx), 256, 512, 2)\n",
    "decoder = Decoder(len(en_word2idx), 256, 512, 2)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "print(f\"Model created on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af77eac94a3b5b0",
   "metadata": {},
   "source": [
    "**üìù Architecture Details:**\n",
    "- **Embedding dimension**: 256 (dense vector representation)\n",
    "- **Hidden dimension**: 512 (LSTM internal state size)\n",
    "- **Layers**: 2 (stacked LSTM layers)\n",
    "- **Dropout**: 0.3 (regularization during training)\n",
    "\n",
    "**üîπ Teacher Forcing:**\n",
    "- During training: Use ground truth previous token\n",
    "- During inference: Use model's own predictions\n",
    "- Helps with training stability and speed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c0ca318e87b2b4",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Training Setup and Loss Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2216e05170078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import time\n",
    "\n",
    "# Loss function ignores padding tokens (index 0)\n",
    "criterion = CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train model for one epoch.\n",
    "    \n",
    "    Process:\n",
    "    1. Forward pass: Arabic ‚Üí English prediction\n",
    "    2. Compute loss: Compare prediction with ground truth\n",
    "    3. Backward pass: Update model parameters\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        src = batch['arabic'].to(device)     # Arabic input\n",
    "        trg = batch['english'].to(device)    # English target\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass with teacher forcing\n",
    "        output = model(src, trg, teacher_forcing_ratio=0.5)\n",
    "        \n",
    "        # Reshape for loss computation\n",
    "        # output: (batch_size, trg_len-1, vocab_size)\n",
    "        # trg: (batch_size, trg_len-1)\n",
    "        output = output[:, 1:].reshape(-1, output.shape[-1])  # Skip <SOS> token\n",
    "        trg = trg[:, 1:].reshape(-1)                          # Skip <SOS> token\n",
    "        \n",
    "        # Compute cross-entropy loss\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44d89e91fc68456",
   "metadata": {},
   "source": [
    "\n",
    "**üìù Loss Computation:**\n",
    "- **CrossEntropyLoss**: Measures prediction accuracy\n",
    "- **ignore_index=0**: Ignores `<PAD>` tokens in loss\n",
    "- **Gradient clipping**: Prevents unstable training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c073d2ba1ce85c6",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9245e0a51ae36f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Training Loop\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "print(\"üö® TRAINING NOTE:\")\n",
    "print(\"This model will OVERFIT because we're using only training data!\")\n",
    "print(\"Purpose: Demonstrate Seq2Seq challenges with Arabic translation\")\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eabe7984f0fd46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    start_time = time.time()\n",
    "    epoch_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch}: Loss = {epoch_loss:.4f}, Time = {time.time() - start_time:.1f}s\")\n",
    "    \n",
    "    # Early stopping when loss gets reasonably low\n",
    "    if epoch_loss < 2.5:\n",
    "        print(\"Target loss reached!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96798fa583ea6b6",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Translation Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bda2a7be99347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Translation Function\n",
    "def translate_sentence(model, sentence, ar_word2idx, en_idx2word, device, max_length=100):\n",
    "    \"\"\"\n",
    "    Translate a single Arabic sentence to English.\n",
    "    \n",
    "    Process:\n",
    "    1. Convert Arabic text to token indices\n",
    "    2. Encode with trained encoder\n",
    "    3. Decode step-by-step (greedy decoding)\n",
    "    4. Convert indices back to English words\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Prepare source sequence\n",
    "        src_seq = text_to_sequence(sentence, ar_word2idx, max_length=60)\n",
    "        src_tensor = torch.tensor(src_seq, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Encode source\n",
    "        hidden, cell = model.encoder(src_tensor)\n",
    "        \n",
    "        # Start decoding with <SOS> token\n",
    "        trg_indexes = [en_word2idx.get('< SOS >', 1)]\n",
    "        \n",
    "        # Generate tokens one by one\n",
    "        for _ in range(max_length):\n",
    "            trg_tensor = torch.tensor([trg_indexes[-1]], dtype=torch.long).unsqueeze(0).to(device)\n",
    "            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
    "            \n",
    "            # Get most likely next token\n",
    "            pred_token = output.argmax(2).item()\n",
    "            trg_indexes.append(pred_token)\n",
    "            \n",
    "            # Stop if <EOS> token is generated\n",
    "            if pred_token == en_word2idx.get('<EOS>', 2):\n",
    "                break\n",
    "        \n",
    "        # Convert indices to words (skip <SOS> and <EOS>)\n",
    "        trg_tokens = [en_idx2word.get(i, '<UNK>') for i in trg_indexes[1:-1]]\n",
    "        return ' '.join(trg_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dff278e041517f9",
   "metadata": {},
   "source": [
    "**üìù Inference Process:**\n",
    "- **Greedy decoding**: Always pick most likely token\n",
    "- **Alternative**: Beam search for better results\n",
    "- **No teacher forcing**: Use model's own predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5271afb79438aa",
   "metadata": {},
   "source": [
    "## üîü Testing Translation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4b4374c0ca5e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test translation\n",
    "test_arabic = subset_data['ar'][0]\n",
    "translation = translate_sentence(model, test_arabic, ar_word2idx, en_idx2word, device)\n",
    "print(f\"Arabic: {test_arabic}\")\n",
    "print(f\"Translation: {translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4cab5d2b2e03a2",
   "metadata": {},
   "source": [
    "**Expected Result:**\n",
    "- Translation quality will be poor due to:\n",
    "  1. **Overfitting** on small training set\n",
    "  2. **Simple tokenization** not handling Arabic morphology\n",
    "  3. **No attention mechanism** losing context in long sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b36638faac6aa62",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ BLEU Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e098f048ecda9256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def compute_bleu_score(reference, candidate, max_n=4):\n",
    "    \"\"\"\n",
    "    Compute BLEU score to measure translation quality.\n",
    "    \n",
    "    BLEU measures n-gram overlap between reference and candidate translations.\n",
    "    Higher scores (0-1) indicate better translation quality.\n",
    "    \"\"\"\n",
    "    def get_ngrams(tokens, n):\n",
    "        if len(tokens) < n:\n",
    "            return []\n",
    "        return [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
    "    \n",
    "    ref_tokens = reference.lower().split()\n",
    "    cand_tokens = candidate.lower().split()\n",
    "    \n",
    "    if len(cand_tokens) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Compute precision for each n-gram level\n",
    "    precisions = []\n",
    "    for n in range(1, max_n + 1):\n",
    "        ref_ngrams = Counter(get_ngrams(ref_tokens, n))\n",
    "        cand_ngrams = Counter(get_ngrams(cand_tokens, n))\n",
    "        \n",
    "        if len(cand_ngrams) == 0:\n",
    "            precisions.append(0.0)\n",
    "            continue\n",
    "            \n",
    "        # Count matches\n",
    "        matches = sum(min(count, ref_ngrams.get(ngram, 0)) \n",
    "                     for ngram, count in cand_ngrams.items())\n",
    "        precision = matches / len(get_ngrams(cand_tokens, n))\n",
    "        precisions.append(precision)\n",
    "    \n",
    "    # Brevity penalty\n",
    "    ref_len = len(ref_tokens)\n",
    "    cand_len = len(cand_tokens)\n",
    "    bp = 1.0 if cand_len > ref_len else math.exp(1 - ref_len / cand_len) if cand_len > 0 else 0.0\n",
    "    \n",
    "    # Final BLEU score\n",
    "    if all(p > 0 for p in precisions):\n",
    "        bleu = bp * math.exp(sum(math.log(p) for p in precisions) / len(precisions))\n",
    "    else:\n",
    "        bleu = 0.0\n",
    "    \n",
    "    return bleu\n",
    "\n",
    "def evaluate_bleu_by_length(model, subset_data, ar_word2idx, en_idx2word, device, num_samples=150):\n",
    "    \"\"\"\n",
    "    Evaluate model performance on different sentence lengths.\n",
    "    Arabic translation difficulty varies significantly with sentence length.\n",
    "    \"\"\"\n",
    "    # Collect samples with their lengths\n",
    "    samples_with_length = []\n",
    "    for i in range(min(num_samples, len(subset_data['ar']))):\n",
    "        arabic_text = subset_data['ar'][i]\n",
    "        english_text = subset_data['en'][i]\n",
    "        text_length = len(english_text.split())\n",
    "        samples_with_length.append((i, arabic_text, english_text, text_length))\n",
    "    \n",
    "    # Sort by length and categorize\n",
    "    samples_with_length.sort(key=lambda x: x[3])\n",
    "    lengths = [x[3] for x in samples_with_length]\n",
    "    short_threshold = lengths[len(lengths)//3]\n",
    "    long_threshold = lengths[2*len(lengths)//3]\n",
    "    \n",
    "    categories = {'short': [], 'medium': [], 'long': []}\n",
    "    \n",
    "    for sample in samples_with_length:\n",
    "        idx, arabic, english, length = sample\n",
    "        if length <= short_threshold:\n",
    "            categories['short'].append(sample)\n",
    "        elif length >= long_threshold:\n",
    "            categories['long'].append(sample)\n",
    "        else:\n",
    "            categories['medium'].append(sample)\n",
    "    \n",
    "    # Evaluate each category\n",
    "    results = {}\n",
    "    for category_name, samples in categories.items():\n",
    "        bleu_scores = []\n",
    "        for idx, arabic_text, reference_english, length in samples:\n",
    "            model_translation = translate_sentence(model, arabic_text, ar_word2idx, en_idx2word, device)\n",
    "            bleu = compute_bleu_score(reference_english, model_translation)\n",
    "            bleu_scores.append(bleu)\n",
    "        \n",
    "        if bleu_scores:\n",
    "            avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "            good_count = sum(1 for s in bleu_scores if s > 0.1)\n",
    "            results[category_name] = {\n",
    "                'avg_bleu': avg_bleu,\n",
    "                'good_count': good_count,\n",
    "                'total': len(bleu_scores)\n",
    "            }\n",
    "    \n",
    "    # Print results\n",
    "    for category in ['short', 'medium', 'long']:\n",
    "        if category in results:\n",
    "            r = results[category]\n",
    "            print(f\"{category.capitalize()}: BLEU = {r['avg_bleu']:.4f}, Good Rate = {r['good_count']/r['total']*100:.1f}%\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c70d63729cbd36c",
   "metadata": {},
   "source": [
    "**üìù BLEU Score Interpretation:**\n",
    "- **0.0-0.1**: Poor translation\n",
    "- **0.1-0.3**: Understandable but low quality\n",
    "- **0.3-0.5**: Good translation\n",
    "- **0.5+**: Excellent translation\n",
    "    - In my humble opinion, this score is useless :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509a5f8b82a8392",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Final Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92baf5af4df2025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run BLEU evaluation\n",
    "print(\"üìä BLEU Score Evaluation (Remember: This is overfitted!)\")\n",
    "print(\"=\" * 50)\n",
    "bleu_results = evaluate_bleu_by_length(model, subset_data, ar_word2idx, en_idx2word, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa683bad8bb5d38",
   "metadata": {},
   "source": [
    "**Expected Results:**\n",
    "- Scores will be artificially high due to overfitting\n",
    "- Real-world performance would be much lower\n",
    "- Demonstrates the challenge of Arabic NLP(Without attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93751bd7a442f6bc",
   "metadata": {},
   "source": [
    "## **üéØ Key Takeaways and Improvements**\n",
    "\n",
    "### **‚ùå Current Issues:**\n",
    "1. **No train/validation/test split** ‚Üí Severe overfitting\n",
    "2. **Simple tokenization** ‚Üí Poor Arabic morphology handling\n",
    "3. **No attention mechanism** ‚Üí Context loss in long sequences\n",
    "4. **Small dataset** ‚Üí Limited generalization\n",
    "5. **No beam search** ‚Üí Suboptimal decoding\n",
    "\n",
    "### **‚úÖ Production Improvements:**\n",
    "1. **Proper data splitting** (80/10/10 train/val/test)\n",
    "2. **Arabic-specific tokenization** (farasa, CAMeL)\n",
    "3. **Attention mechanism** or **Transformer architecture**\n",
    "4. **Larger dataset** with domain diversity\n",
    "5. **Advanced decoding** (beam search, length penalties)\n",
    "6. **Regularization techniques** (dropout, weight decay)\n",
    "\n",
    "### **üîπ Arabic-Specific Challenges:**\n",
    "- **Rich morphology**: One root ‚Üí hundreds of word forms\n",
    "    - ŸÅÿπŸÑ: ŸÅÿπŸàŸÑ ŸÅÿßÿπŸÑÿå ŸÖŸÅÿπŸàŸÑÿå ŸÅÿßÿπŸÑŸàŸÜÿå ŸÅÿßÿπŸÑŸäŸÜÿå ÿ™ŸÅÿπŸÑÿå ŸäŸÅÿπŸÑŸÜÿå ÿ™ŸÅÿπŸÑŸÜ... ÿßŸÑÿÆ\n",
    "- **Agglutination**: Prefixes and suffixes change meaning\n",
    "- **Diacritics**: Optional vowel marks affect pronunciation\n",
    "- **Right-to-left script**: Processing order considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859b178aaa0b8d54",
   "metadata": {},
   "source": [
    "Contributed by: Ali Habibullah\n",
    "> I am not proud of this work  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
