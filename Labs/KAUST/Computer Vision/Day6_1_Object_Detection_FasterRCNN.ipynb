{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "936c6c87",
   "metadata": {
    "id": "936c6c87"
   },
   "source": [
    "![image.png](https://i.imgur.com/a3uAqnb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824b7295",
   "metadata": {
    "papermill": {
     "duration": 0.00414,
     "end_time": "2025-02-02T15:14:30.090143",
     "exception": false,
     "start_time": "2025-02-02T15:14:30.086003",
     "status": "completed"
    },
    "tags": [],
    "id": "824b7295"
   },
   "source": [
    "# **üöÄ Object Detection with Faster R-CNN**\n",
    "In this lab, we will:\n",
    "\n",
    "‚úÖ Build a **custom Dataset class** for **Pascal VOC dataset**  \n",
    "‚úÖ Use a **pretrained Faster R-CNN model** for object detection  \n",
    "‚úÖ Train and evaluate the model  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df1282f",
   "metadata": {
    "papermill": {
     "duration": 0.003161,
     "end_time": "2025-02-02T15:14:30.096945",
     "exception": false,
     "start_time": "2025-02-02T15:14:30.093784",
     "status": "completed"
    },
    "tags": [],
    "id": "8df1282f"
   },
   "source": [
    "## **1Ô∏è‚É£ Dataset Class**\n",
    "We use the **Pascal VOC 2007 dataset**, which contains images with **bounding boxes and labels**.  \n",
    "PyTorch provides a built-in dataset loader: `torchvision.datasets.VOCDetection`."
   ]
  },
  {
   "cell_type": "code",
   "id": "4f60e0b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:14:30.104558Z",
     "iopub.status.busy": "2025-02-02T15:14:30.104288Z",
     "iopub.status.idle": "2025-02-02T15:15:32.384726Z",
     "shell.execute_reply": "2025-02-02T15:15:32.383763Z"
    },
    "papermill": {
     "duration": 62.28615,
     "end_time": "2025-02-02T15:15:32.386383",
     "exception": false,
     "start_time": "2025-02-02T15:14:30.100233",
     "status": "completed"
    },
    "tags": [],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4f60e0b8",
    "outputId": "1ee5fc1b-4ed5-4a8c-fbc4-2bae0c3a389b"
   },
   "source": [
    "### **üîπ Load & Transform Dataset**\n",
    "import torchvision\n",
    "from torchvision.datasets import VOCDetection\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# Define dataset path and transformations\n",
    "data_path = \"./VOC_data/\"\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "# Load Pascal VOC dataset (train & test)\n",
    "train_dataset = VOCDetection(root=data_path, year='2007', image_set='train', download=True, transform=transform)\n",
    "test_dataset = VOCDetection(root=data_path, year='2007', image_set='test', download=True, transform=transform)\n",
    "\n",
    "# Custom collate function to handle variable number of bounding boxes\n",
    "def custom_collate_fn(data):\n",
    "    return tuple(zip(*data))\n",
    "\n",
    "# Create DataLoaders                 (Tip: Use lower batch size if encountered Out-of-Memory (OOM) error in Training)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=2, collate_fn=custom_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=2, collate_fn=custom_collate_fn)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "63daca94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:15:32.425082Z",
     "iopub.status.busy": "2025-02-02T15:15:32.424677Z",
     "iopub.status.idle": "2025-02-02T15:15:32.429156Z",
     "shell.execute_reply": "2025-02-02T15:15:32.428465Z"
    },
    "papermill": {
     "duration": 0.024824,
     "end_time": "2025-02-02T15:15:32.430374",
     "exception": false,
     "start_time": "2025-02-02T15:15:32.405550",
     "status": "completed"
    },
    "tags": [],
    "id": "63daca94"
   },
   "source": [
    "# Mappings of label names (found in dataset annotation) to integer IDs (or classes) which we will feed to the model\n",
    "voc_classes = {\n",
    "    \"aeroplane\": 0,\n",
    "    \"bicycle\": 1,\n",
    "    \"bird\": 2,\n",
    "    \"boat\": 3,\n",
    "    \"bottle\": 4,\n",
    "    \"bus\": 5,\n",
    "    \"car\": 6,\n",
    "    \"cat\": 7,\n",
    "    \"chair\": 8,\n",
    "    \"cow\": 9,\n",
    "    \"diningtable\": 10,\n",
    "    \"dog\": 11,\n",
    "    \"horse\": 12,\n",
    "    \"motorbike\": 13,\n",
    "    \"person\": 14,\n",
    "    \"pottedplant\": 15,\n",
    "    \"sheep\": 16,\n",
    "    \"sofa\": 17,\n",
    "    \"train\": 18,\n",
    "    \"tvmonitor\": 19,\n",
    "}\n",
    "\n",
    "#  Reverse of label to class id mapping. needed because the model predictions will be ids and we need to change it to label to visualize it.\n",
    "reverse_voc_classes = {v: k for k, v in voc_classes.items()}\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a7efd317",
   "metadata": {
    "papermill": {
     "duration": 0.018153,
     "end_time": "2025-02-02T15:15:32.467163",
     "exception": false,
     "start_time": "2025-02-02T15:15:32.449010",
     "status": "completed"
    },
    "tags": [],
    "id": "a7efd317"
   },
   "source": [
    "### **üîπ Why Do We Need a Custom `collate_fn`?**\n",
    "Unlike classification datasets, where each image has a **fixed shape and label**, object detection images have **variable numbers of bounding boxes**.  \n",
    "\n",
    "- The default `collate_fn` (which applies `torch.stack`) **doesn't work**, since bounding box tensors have different shapes.  \n",
    "- Instead, we **return a tuple** that **keeps individual image-label pairs separate**.\n",
    "\n",
    "##### Before using custom collate_fn:\n",
    "```python\n",
    "data = [\n",
    "    (image1, dict1),  \n",
    "    (image2, dict2),\n",
    "    ...  \n",
    "]\n",
    "```\n",
    "##### After:\n",
    "```python\n",
    "images_tuple = (image1, image2, ...)  \n",
    "targets_tuple = (dict1, dict2, ...)   \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cd40bf",
   "metadata": {
    "papermill": {
     "duration": 0.019458,
     "end_time": "2025-02-02T15:15:32.504979",
     "exception": false,
     "start_time": "2025-02-02T15:15:32.485521",
     "status": "completed"
    },
    "tags": [],
    "id": "47cd40bf"
   },
   "source": [
    "## **2Ô∏è‚É£ Model Class**\n",
    "We use a **pretrained Faster R-CNN with a MobileNetV3-Large backbone**.\n",
    "\n",
    "### **üîπ Modify the Model**\n",
    "- The default model is trained on **COCO dataset** with **91 classes**.\n",
    "- We modify the classifier to detect **20 Pascal VOC classes**.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "524ed7ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:15:32.542958Z",
     "iopub.status.busy": "2025-02-02T15:15:32.542626Z",
     "iopub.status.idle": "2025-02-02T15:15:34.148855Z",
     "shell.execute_reply": "2025-02-02T15:15:34.147973Z"
    },
    "papermill": {
     "duration": 1.627058,
     "end_time": "2025-02-02T15:15:34.150216",
     "exception": false,
     "start_time": "2025-02-02T15:15:32.523158",
     "status": "completed"
    },
    "scrolled": true,
    "tags": [],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "524ed7ea",
    "outputId": "868957c7-2c46-4b48-e253-3fec216544e2"
   },
   "source": [
    "import torchvision\n",
    "\n",
    "# Load pretrained Faster R-CNN model\n",
    "model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
    "\n",
    "# Change number of output classes to match Pascal VOC dataset\n",
    "num_classes = 20  # Pascal VOC has 20 object classes\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features  # Input features for predictor\n",
    "\n",
    "# Replace final layer with new predictor\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "\n",
    "# Freeze the backbone and just finetune the head (You can finetune the whole model, but it'd take time and resources)\n",
    "model.requires_grad_(False)\n",
    "model.roi_heads.box_predictor = model.roi_heads.box_predictor.requires_grad_(True)\n",
    "\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "def5003f",
   "metadata": {
    "papermill": {
     "duration": 0.01897,
     "end_time": "2025-02-02T15:15:34.188986",
     "exception": false,
     "start_time": "2025-02-02T15:15:34.170016",
     "status": "completed"
    },
    "tags": [],
    "id": "def5003f"
   },
   "source": [
    "## **3Ô∏è‚É£ Training and Validation Loops**\n",
    "### **üîπ Training Loop**\n",
    "- The model takes **images & targets** and computes **losses internally**. No need to define the loss.\n",
    "- We only need to **backpropagate and update the optimizer**.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6ed05f2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:15:34.228164Z",
     "iopub.status.busy": "2025-02-02T15:15:34.227920Z",
     "iopub.status.idle": "2025-02-02T15:15:34.233974Z",
     "shell.execute_reply": "2025-02-02T15:15:34.233345Z"
    },
    "papermill": {
     "duration": 0.027129,
     "end_time": "2025-02-02T15:15:34.235141",
     "exception": false,
     "start_time": "2025-02-02T15:15:34.208012",
     "status": "completed"
    },
    "tags": [],
    "id": "6ed05f2b"
   },
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for images, targets in tqdm(dataloader):\n",
    "        images = list(img.to(device) for img in images)\n",
    "\n",
    "        # Convert targets\n",
    "        for target in targets:\n",
    "            boxes = []\n",
    "            labels = []\n",
    "            for obj in target['annotation']['object']:\n",
    "                label = obj['name']\n",
    "                box = obj['bndbox']\n",
    "                xmin, ymin, xmax, ymax = [int(box[k]) for k in ['xmin', 'ymin', 'xmax', 'ymax']]\n",
    "                boxes.append(torch.Tensor([xmin, ymin, xmax, ymax]).to(device))\n",
    "                labels.append(voc_classes[label])\n",
    "\n",
    "            target['boxes'] = torch.stack(boxes)\n",
    "            target['labels'] = torch.Tensor(labels).type(torch.int64).to(device)\n",
    "\n",
    "        # Compute losses\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())  # Sum all losses\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += losses.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4c407612",
   "metadata": {
    "papermill": {
     "duration": 0.01926,
     "end_time": "2025-02-02T15:15:34.273816",
     "exception": false,
     "start_time": "2025-02-02T15:15:34.254556",
     "status": "completed"
    },
    "tags": [],
    "id": "4c407612"
   },
   "source": [
    "### **üîπ Validation Loop**\n",
    "#### **üîπ How Do We Evaluate Object Detection Models?**\n",
    "To evaluate object detection models like **Faster R-CNN**, we need to measure **how well the predicted bounding boxes match the ground truth boxes**.\n",
    "\n",
    "![image.png](https://i.imgur.com/MDFxFMX.png)\n",
    "\n",
    "---\n",
    "\n",
    "#### **üìå Intersection over Union (IoU)**\n",
    "‚úÖ We consider a detection **correct** if the predicted box **overlaps significantly** with the ground truth box.  \n",
    "‚úÖ This is measured using **Intersection over Union (IoU)**, which calculates the **ratio of overlap** between the two boxes.\n",
    "\n",
    "$$\n",
    "IoU = \\frac{\\text{Area of Overlap}}{\\text{Area of Union}}\n",
    "$$\n",
    "\n",
    "üöÄ **Higher IoU = Better Detection!**  \n",
    "\n",
    "\n",
    "![image.png](https://i.imgur.com/yNNhjwr.png)\n",
    "\n",
    "---\n",
    "\n",
    "#### **üìå What is mAP@0.5:0.95?**\n",
    "mAP (**mean Average Precision**) is the most commonly used **metric for object detection**.\n",
    "\n",
    "üîπ **mAP@0.5:0.95** means we compute the **average precision** at **different IoU thresholds** from **0.5 to 0.95**, increasing in steps of **0.05**.\n",
    "\n",
    "- **IoU ‚â• 0.5** ‚Üí Loose match  \n",
    "- **IoU ‚â• 0.75** ‚Üí Stricter match  \n",
    "- **IoU ‚â• 0.95** ‚Üí Extremely strict match  \n",
    "\n",
    "**mAP@0.5:0.95** takes the **average of all these values**, giving us a single number that represents how well the model performs **across different difficulty levels**.\n"
   ]
  },
  {
   "metadata": {
    "id": "5cd2ac986ab9db3c"
   },
   "cell_type": "code",
   "source": [
    "from IPython.display import clear_output\n",
    "!pip install torchmetrics\n",
    "clear_output()"
   ],
   "id": "5cd2ac986ab9db3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "81209ee3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:15:34.313195Z",
     "iopub.status.busy": "2025-02-02T15:15:34.312948Z",
     "iopub.status.idle": "2025-02-02T15:15:38.090998Z",
     "shell.execute_reply": "2025-02-02T15:15:38.090302Z"
    },
    "papermill": {
     "duration": 3.799644,
     "end_time": "2025-02-02T15:15:38.092514",
     "exception": false,
     "start_time": "2025-02-02T15:15:34.292870",
     "status": "completed"
    },
    "tags": [],
    "id": "81209ee3"
   },
   "source": [
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "# Initialize metric\n",
    "metric = MeanAveragePrecision(iou_thresholds=[0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95])\n",
    "\n",
    "def validate(model, dataloader, device):\n",
    "    \"\"\"Evaluates the model using mAP@0.5:0.95.\"\"\"\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(dataloader):\n",
    "            images = [img.to(device) for img in images]\n",
    "            preds = model(images)\n",
    "\n",
    "            # Convert predictions to correct format\n",
    "            processed_preds = []\n",
    "            for pred in preds:\n",
    "                processed_preds.append({\n",
    "                    \"boxes\": pred[\"boxes\"].cpu(),\n",
    "                    \"scores\": pred[\"scores\"].cpu(),\n",
    "                    \"labels\": pred[\"labels\"].cpu()\n",
    "                })\n",
    "\n",
    "            # Convert ground truth targets\n",
    "            processed_targets = []\n",
    "            for target in targets:\n",
    "                gt_boxes = []\n",
    "                gt_labels = []\n",
    "                for obj in target['annotation']['object']:\n",
    "                    label = obj['name']\n",
    "                    box = obj['bndbox']\n",
    "                    xmin, ymin, xmax, ymax = [int(box[k]) for k in ['xmin', 'ymin', 'xmax', 'ymax']]\n",
    "                    gt_boxes.append([xmin, ymin, xmax, ymax])\n",
    "                    gt_labels.append(voc_classes[label])\n",
    "\n",
    "                processed_targets.append({\n",
    "                    \"boxes\": torch.tensor(gt_boxes).cpu(),\n",
    "                    \"labels\": torch.tensor(gt_labels).cpu()\n",
    "                })\n",
    "\n",
    "            # Update metric\n",
    "            metric.update(processed_preds, processed_targets)\n",
    "\n",
    "    return metric.compute()  # Compute final mAP scores"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bc0abbe7",
   "metadata": {
    "papermill": {
     "duration": 0.01929,
     "end_time": "2025-02-02T15:15:38.131855",
     "exception": false,
     "start_time": "2025-02-02T15:15:38.112565",
     "status": "completed"
    },
    "tags": [],
    "id": "bc0abbe7"
   },
   "source": [
    "## **4Ô∏è‚É£ Running Training & Validation**"
   ]
  },
  {
   "cell_type": "code",
   "id": "35c395d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:15:38.171378Z",
     "iopub.status.busy": "2025-02-02T15:15:38.170976Z",
     "iopub.status.idle": "2025-02-02T15:48:25.956955Z",
     "shell.execute_reply": "2025-02-02T15:48:25.955928Z"
    },
    "papermill": {
     "duration": 1967.807205,
     "end_time": "2025-02-02T15:48:25.958493",
     "exception": false,
     "start_time": "2025-02-02T15:15:38.151288",
     "status": "completed"
    },
    "tags": [],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35c395d8",
    "outputId": "e8da0b6b-b3ac-4fcb-9126-162a5ee0fa18"
   },
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "num_epochs = 10  # Set number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "    mAP_results = validate(model, test_loader, device)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.4f}\")\n",
    "    print(f\"mAP@0.5:0.95 for Test: {mAP_results['map']:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d0a7d245",
   "metadata": {
    "papermill": {
     "duration": 0.711607,
     "end_time": "2025-02-02T15:48:27.316678",
     "exception": false,
     "start_time": "2025-02-02T15:48:26.605071",
     "status": "completed"
    },
    "tags": [],
    "id": "d0a7d245"
   },
   "source": [
    "## **5Ô∏è‚É£ Visualizing Predictions vs. Ground Truth**\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "import torchvision.transforms.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches"
   ],
   "metadata": {
    "id": "qetjqo8aClQA"
   },
   "id": "qetjqo8aClQA",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7e1447c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T15:48:30.059554Z",
     "iopub.status.busy": "2025-02-02T15:48:30.059191Z",
     "iopub.status.idle": "2025-02-02T15:48:31.705801Z",
     "shell.execute_reply": "2025-02-02T15:48:31.704571Z"
    },
    "papermill": {
     "duration": 2.40335,
     "end_time": "2025-02-02T15:48:31.754464",
     "exception": false,
     "start_time": "2025-02-02T15:48:29.351114",
     "status": "completed"
    },
    "tags": [],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 624
    },
    "id": "7e1447c4",
    "outputId": "99acb42d-af58-47b5-8333-dba431434701"
   },
   "source": [
    "# Select 3 random test images\n",
    "test_indices = [2777, 4742, 777]\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Create figure with 5√ó2 subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(30, 60))\n",
    "axes = axes.ravel()  # Flatten axes for easy iteration\n",
    "\n",
    "for i, idx in enumerate(test_indices):\n",
    "    test_img, test_target = test_dataset[idx]\n",
    "\n",
    "    # Extract Ground Truth Boxes & Labels\n",
    "    gt_boxes = []\n",
    "    gt_annotations = []\n",
    "    for obj in test_target['annotation']['object']:\n",
    "        box = obj['bndbox']\n",
    "        xmin, ymin, xmax, ymax = [int(box[k]) for k in ['xmin', 'ymin', 'xmax', 'ymax']]\n",
    "        gt_boxes.append([xmin, ymin, xmax, ymax])\n",
    "        gt_annotations.append(obj['name'])\n",
    "\n",
    "    # Run Model on Test Image\n",
    "    with torch.no_grad():\n",
    "        pred = model([test_img.to(device)])\n",
    "\n",
    "    pred = pred[0]\n",
    "\n",
    "    # Extract Predictions\n",
    "    pred_boxes = pred['boxes'].cpu()\n",
    "    pred_annotations = pred['labels'].cpu()\n",
    "    pred_scores = pred['scores'].cpu()\n",
    "\n",
    "    # Apply Confidence Threshold (Only keep predictions with score ‚â• 0.8)\n",
    "    valid_mask = pred_scores >= 0.8\n",
    "    pred_annotations = pred_annotations[valid_mask]\n",
    "    pred_boxes = pred_boxes[valid_mask]\n",
    "\n",
    "    # Convert Predicted Labels from Numeric to Class Names\n",
    "    pred_annotations = [reverse_voc_classes[val.item()] for val in pred_annotations]\n",
    "\n",
    "    # Overlay GT & Predictions on Image\n",
    "    img = F.to_pil_image(test_img)\n",
    "    ax = axes[i]\n",
    "    ax.imshow(img)\n",
    "\n",
    "    # Plot Ground Truth in RED\n",
    "    for bbox, annotation in zip(gt_boxes, gt_annotations):\n",
    "        x_min, y_min, x_max, y_max = bbox\n",
    "        width = x_max - x_min\n",
    "        height = y_max - y_min\n",
    "\n",
    "        rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x_min, y_min - 5, annotation, color='r', fontsize=24, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "    # Plot Predictions in GREEN\n",
    "    for bbox, annotation in zip(pred_boxes, pred_annotations):\n",
    "        x_min, y_min, x_max, y_max = bbox\n",
    "        width = x_max - x_min\n",
    "        height = y_max - y_min\n",
    "\n",
    "        rect = patches.Rectangle((x_min, y_min), width, height, linewidth=2, edgecolor='g', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x_min, y_min - 5, annotation, color='g', fontsize=24, bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f\"Image {idx}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bd0f36d4",
   "metadata": {
    "id": "bd0f36d4"
   },
   "source": [
    "### Contributed by: Mohamed Eltayeb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d695b2",
   "metadata": {
    "id": "d4d695b2"
   },
   "source": [
    "![image.png](https://i.imgur.com/a3uAqnb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a12c26",
   "metadata": {
    "id": "a2a12c26"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2046.963277,
   "end_time": "2025-02-02T15:48:34.514608",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-02T15:14:27.551331",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
